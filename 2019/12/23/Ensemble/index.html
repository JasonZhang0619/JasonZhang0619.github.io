<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Jiaxin Zhang" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="In statistics and machine learning, Ensemble method use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alon">
<meta name="keywords" content="Machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Ensemble learning">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;index.html">
<meta property="og:site_name" content="Jiaxin Zhang">
<meta property="og:description" content="In statistics and machine learning, Ensemble method use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alon">
<meta property="og:locale" content="en">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;Notation.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;Ensemble.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;bagging.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;rf.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;gb.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;xgb.jpg">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;ada.jpg">
<meta property="og:updated_time" content="2020-01-13T23:18:26.567Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;23&#x2F;Ensemble&#x2F;Notation.jpg">

<link rel="canonical" href="http://yoursite.com/2019/12/23/Ensemble/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Ensemble learning | Jiaxin Zhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiaxin Zhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">MSc in Statistical Machine Learning @ UofA</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/JasonZhang0619" class="github-corner" title="GitHub" aria-label="GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/23/Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="Curious, Open-eyed, Humble, Prepared, Sensitive.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ensemble learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-23 22:22:08" itemprop="dateCreated datePublished" datetime="2019-12-23T22:22:08-07:00">2019-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-13 16:18:26" itemprop="dateModified" datetime="2020-01-13T16:18:26-07:00">2020-01-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>In statistics and machine learning, <a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener">Ensemble method</a> use <strong>multiple learning algorithms</strong> to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.<br>This post is mainly about the theory behind these big names. </p>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><ul>
<li>data<br>$\mathcal{X}$ input space ($p$ dimensions) $\mathcal{Y}$ output space<br>$\mathbf{x}$ input variable $\mathbf{x} \in \mathcal{X}$<br>$n$ train sample size<br>$\mathbf{x}_i, y_i$ training input and output, $i= 1, \dots, n$<br>$w_i$ weight for sample $\mathbf{x}_i, y_i$<br>$\mathbf{D}=(\mathbf{X},\mathbf{y})$ is the design matrix $(\mathbf{x}_1,\dots, \mathbf{x}_n)^T$ and output vector $(y_1,\dots, y_n)^T$<br>$\mathcal{RC}(D)$ random choice from dataset $D$ <strong>with no replacement</strong><br>$\mathcal{RS}(D)$ random sampling from dataset $D$ <strong>with replacement</strong></li>
<li>learners<br>$f: \mathcal{X} \to \mathcal{Y}$ arbitrary function<br>$\mathcal{H}$ base learner space<br>$g: \mathcal{X} \to \mathcal{Y}$ and $g \in \mathcal{H}$ a base learner.<br>$g(\mathbf{x}|\mathbf{D})$ a base learner trained with \mathbf{D}<br>$K$ number of base learners<br>$\alpha_k$ weight for $g_k$, $k=1,\dots, K$</li>
<li>ensemble<br>$f_{*}^t: \mathcal{X} \to \mathcal{Y}$ The * ensemble with first $t$ base learners</li>
<li>loss function<br>$L(y,\hat{y})$ loss function comparing $y,\hat{y}$<br>$\mathbf{L}(*|\mathbf{D})$ loss target for * given $\mathbf{D}$<br>$\Omega(f)$ regularization function on $f$</li>
<li>CART<br>$R_j^t$ the j-th region partitioned by CART $g_t$<br>$J_t$ the total number of partitions  by CART $g_t$</li>
</ul>
<img src="/2019/12/23/Ensemble/Notation.jpg" class="" width="500" title="notation for figs">
<hr>
<p>Given training data $\mathbf{D}$, regression task aims to find a regression function </p>
<script type="math/tex; mode=display">y=g(\mathbf{x}|\mathbf{D})</script><p>when $\mathcal{Y} = \mathbf{R}$. Similarly, classification results in a multivariate function that estimate the probabilities (or equivalent) of a given input belonging to every class when $\mathcal{Y}$ is a finite set of categories and returns the category with largest prob.</p>
<script type="math/tex; mode=display">y=\arg max_y P(\mathbf{x}|y,\mathbf{D}) = which\ max \mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>, $\mathcal{P}(\mathbf{x}|\mathbf{D})=(P(\mathbf{x}|1,\mathbf{D}),\dots, P(\mathbf{x}||\mathcal{Y}|,\mathbf{D}))^T$ denoted as $g(\mathbf{x}|\mathbf{D})$ which returns the vector of probs for all categories $1,\dots,|\mathcal{Y}|$. Hense, classification task can also be taken as multi-variate linear regression where the response variable is $\mathcal{P}(\mathbf{x}|\mathbf{D})$.</p>
<p>Either for regression or classification, we build multiple models and these models (weak tho) are called base learner. Ensemble learning integrated these base learner by taking a linear additive combination of $g_k(\mathbf{x}|D_k)$.</p>
<script type="math/tex; mode=display">f_{ensemble}=\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}|D_k)</script><img src="/2019/12/23/Ensemble/Ensemble.jpg" class="" width="400" title="ensemble">
<h1 id="Hard-amp-soft-voting"><a href="#Hard-amp-soft-voting" class="headerlink" title="Hard &amp; soft voting"></a>Hard &amp; soft voting</h1><p>Voting is an idea specific for classification. Every base learner is trained on the <strong>whole dataset</strong> $D_k=\mathbf{D}$ and averaged with <strong>equal weight</strong> $\alpha_k=1/k$.</p>
<script type="math/tex; mode=display">f_{voting}=\sum_{k=1}^{K}\frac{1}{K}g_k(\mathbf{x}|\mathbf{D})=mean(g_k(\mathbf{x}|\mathbf{D}))</script><p>The soft voting is directly taking the average of prob vectors</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>while the hard voting instead takes the average of one-hot vector transformed from probs vectors.</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathbf{1}(\mathcal{P}(\mathbf{x}|\mathbf{D})==\max(\mathcal{P}(\mathbf{x}|\mathbf{D}))</script><p>For example, in one binary classification task we ensemble three classifiers, the prob that an observation belongs to class 1 equal (0.45, 0.45, 0.9). With soft voting the average prob is (0.45 + 0.45 + 0.9)/3 = 0.6 while hard voting will return the average prob equal (0 + 0 + 1)/3 = 0.33.</p>
<h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bootstrap aggregating, often abbreviated as bagging, have each base learner in the ensemble vote with <strong>equal weight</strong> $\alpha_k=1/k$. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn <strong>subset of the training set</strong> $D_k=\mathcal{RS}(\mathbf{D})$. (In statistics, bootstrapping is any test or metric that relies on <strong>random sampling with replacement</strong>.)</p>
<script type="math/tex; mode=display">f_{bagging}=mean(g_k(\mathbf{x}|\mathcal{RS}(\mathbf{D})))</script><h2 id="OOB-score"><a href="#OOB-score" class="headerlink" title="OOB score??"></a>OOB score??</h2><img src="/2019/12/23/Ensemble/bagging.jpg" class="" width="400" title="bagging">
<h2 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h2><p>Random forest consists of more randomness. With <strong>base learner space $\mathcal{H}$ being CART</strong>(classification and regression tree), not only is <strong>training set randomly sampled</strong> piece by piece, but also a <strong>features subset is selected randomly</strong> </p>
<script type="math/tex; mode=display">D_k=\mathcal{RS}((\mathbf{X}_{:,\mathcal{RC}(1\dots,p)},\mathbf{y}))</script><img src="/2019/12/23/Ensemble/rf.jpg" class="" width="500" title="random forrest">
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Boosting involves <strong>incrementally</strong> building an ensemble by training each new model instance to <strong><del>emphasize</del> correct the training instances mis-classified by previous models</strong>. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data.<br>Target is to reduce the Loss function</p>
<script type="math/tex; mode=display">\mathbf{L}(f|\mathbf{D}) = \sum_i^{n}L(y_i,f(\mathbf{x}_i))</script><script type="math/tex; mode=display">f_{boost}=\arg \min \mathbf{L}(\mathbf{\alpha},\mathbf{g}|\mathbf{D}) = \arg \min_{\mathbf{\alpha},\mathbf{g}}\sum_i^{n}L(y_i,\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}_i))</script><p>It is computationally impossible to optimize the target function with $K$ learners <script type="math/tex">g_k</script> and weights $\alpha_k$ at the same time. Hence, we take the idea of <strong>greedy approach</strong> and optimize the target function <strong>sequentially</strong>. We start from training a weak learner $g_1$ and add more learners one by one to further reduce the loss function. </p>
<p>Given first $t-1$ learners ensemble optimized <script type="math/tex">f_{boost}^{t-1}=\sum_{k=1}^{t-1}\alpha_k g_k(X)</script>, the k-th learner can be obtained by optimizing:</p>
<script type="math/tex; mode=display">\min_{\alpha_t}\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i))</script><script type="math/tex; mode=display">\alpha_t, g_t = \arg \min_{\alpha, g} \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha g(\mathbf{x}_i))</script><h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><p>The idea of gradient boosting is to apply a <strong>steepest descent</strong> step to this minimization problem. For arbitrary <strong>differentiable loss function</strong> $L$, </p>
<script type="math/tex; mode=display">\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}} = f(y,f_{boost}^{t-1}(\mathbf{x})) \doteq r^t</script><p>This gradient value is called pseudo-residuals <script type="math/tex">r^t</script> and it can be calculated for every sample $r_i^t$. In regression task, when loss function is MSE, pseudo-residual is actually real residual.</p>
<script type="math/tex; mode=display">L(y,g)=(y-g)^2 \\ \frac{\partial L(y,g)}{\partial g} = -2(y-g)</script><p>In classification task,</p>
<script type="math/tex; mode=display">?</script><p>Let the t-th learner <script type="math/tex">g_t = - r_i^t</script>, adding $g_t$ to must reduce the loss function</p>
<script type="math/tex; mode=display">\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) \ge \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)- \alpha_t r_i^t)</script><p>Ideally we wish to find a learner such that <script type="math/tex">g_t = - r_i^t</script> is true every where, but this is even not possible for all observed samples or worthwhile since this is just a intermediate step during iterations. However, we can still use pseudo-residuals to guide the training of <script type="math/tex">g_t</script>. More concretely, we take pseudo-residuals of <script type="math/tex">f_{boost}^{t-1}</script> as response variable.</p>
<script type="math/tex; mode=display">D_t=\{(\mathbf{x}_i, r_i^t)\}</script><p>After training $g_k$ we still need to determine a good weight $\alpha_k$ for it. It might be impossible to find a closed form of optimal solution but there are still ways to find one good enough. Line search is one of the basic ways.</p>
<img src="/2019/12/23/Ensemble/gb.jpg" class="" width="500"> 
<p>In general, Gradient Boosting includes two stages in each iteration: </p>
<ol>
<li>k-th learner <script type="math/tex">g_t</script> is trained by pseudo-residuals (training set <script type="math/tex">D_t=\{(\mathbf{x}_i, r_i^t)\}</script>) </li>
<li>Line search <script type="math/tex">\alpha_t = \arg \min_{\alpha} \sum_i^{n}(y_i,f_{GB}^{t-1}+\alpha g_t)</script></li>
</ol>
<h3 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting Decision Tree (GBDT)"></a>Gradient Boosting Decision Tree (GBDT)</h3><p>Take decision tree (CART) being weak learner space as example. For this special case, Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner. (by introducing <strong>leaf-wise weight</strong> on base tree learner) Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region. Using the indicator notation, the output of <script type="math/tex">g_t</script> for input <script type="math/tex">\mathbf{x}</script> can be written as the sum: </p>
<script type="math/tex; mode=display">g_t(\mathbf{x})=\sum_{j=1}^{J_t}b_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p>where $b_j^t$ is the value predicted in the region $R_j^t$. (The partitions for different tree $g_t$ should be distinct otherwise there is no potential to improve the boosting function.) Then Line search </p>
<script type="math/tex; mode=display">\alpha_t = \arg \min_{\alpha} \sum_i^n L(y_i,f_{GB}^{t-1}+\alpha g_t)</script><p>Friedman proposes to modify this algorithm so that it chooses a separate optimal value $ \alpha_j^t$ for each of the tree($g_t$)`s regions $R_j^t$, rather than a single $\alpha_t$ for the whole tree. He calls the modified algorithm “TreeBoost”. This is equivalent to training the leaves weight $b_j^t$ according to the overall loss function rather than taking the average or voting result as weight (what CARTs do). However, the <strong>partition of $R_j^t$ is still determined by rules used in CART, which is irrelevant to loss function</strong>.</p>
<script type="math/tex; mode=display">\alpha = \arg \min_{\alpha} \sum_j^{J_t} \sum_{\mathbf{x}_i \in \mathbf{R}_j^t} L(y_i,f_{GB}^{t-1}+\alpha_j^t (\mathbf {1} _{R_j^t}(\mathbf{x}_i)))</script><h3 id="Stochastic-Gradient-Boosting-SGB"><a href="#Stochastic-Gradient-Boosting-SGB" class="headerlink" title="Stochastic Gradient Boosting (SGB)"></a>Stochastic Gradient Boosting (SGB)</h3><p>Friedman proposed a minor modification to the algorithm, motivated by bootstrap aggregation (“bagging”) method at each iteration of the algorithm, a base learner should be fit on a <strong>subsample of the training set</strong> drawn at random without replacement. It not only introduces randomness into the algorithm and help <strong>prevent overfitting</strong>, but also makes <strong>training faster</strong>, because regression trees have to be fit to smaller datasets at each iteration. He obtained that 0.5 ≤ f ≤ 0.8  leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.</p>
<h2 id="XGboost"><a href="#XGboost" class="headerlink" title="XGboost"></a>XGboost</h2><p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener">XGBoost</a> stands for Extreme Gradient Boosting; it is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best <strong>tree </strong>model. It employs a number of nifty tricks that make it exceptionally successful, particularly with structured data. </p>
<ul>
<li>computing second-order gradients, i.e. second partial derivatives of the loss function (similar to Newton’s method), which provides more information about the direction of gradients and how to get to the minimum of our loss function.</li>
<li>advanced regularization (L1 &amp; L2), which improves model generalization.</li>
<li>training is very fast and can be parallelized / distributed across clusters.</li>
</ul>
<h3 id="Regularization-on-Model-Complexity"><a href="#Regularization-on-Model-Complexity" class="headerlink" title="Regularization on Model Complexity"></a>Regularization on Model Complexity</h3><p>Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region</p>
<script type="math/tex; mode=display">g(\mathbf{x})=\sum_{j=1}^{J}b_j\mathbf {1} _{R_j}(\mathbf{x})</script><p>In XGBoost, we define the complexity as</p>
<script type="math/tex; mode=display">\Omega(g)=\eta J+\frac{1}{2}\lambda \sum_{j=1}^{J}b_j^2</script><p>First term $\eta J$ is the penalty on the number of leaves $J$, which is one source of complexity, and $\eta$ is a hyper-parameter to control the strength of this penalty. Complexity also comes from $b_j$ and the second term is the $L_2$ regularization on $b_j$, which is quite similar to Ridge regression. </p>
<p>Of course, there is more than one way to define the complexity, but this one works well in practice. The regularization is one part most tree packages treat less carefully, or simply ignore. This was because the traditional treatment of tree learning only emphasized improving impurity, while the complexity control was left to heuristics. </p>
<p>The objective function for boosting at each iteration can be modified as below</p>
<script type="math/tex; mode=display">\min_{\alpha_t, g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i)) + \Omega(\alpha_tg_t)</script><p>A CART $g_t$ weighted by $\alpha_t$ is still a CART,</p>
<script type="math/tex; mode=display">\alpha_tg_t(\mathbf{x})=\sum_{j=1}^{J}\alpha_tb_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p><strong>Since in following Steps, we do not train a CART by information gain rule or any principle independent from loss function, we can cancel the weight $\alpha_t$</strong></p>
<script type="math/tex; mode=display">\min_{g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+g_t(\mathbf{x}_i)) + \Omega(g_t)</script><h3 id="Tayler-expansion-approximation"><a href="#Tayler-expansion-approximation" class="headerlink" title="Tayler expansion approximation"></a>Tayler expansion approximation</h3><p>One key specialty of XGboost is that it approximates this loss function by taking Taylor expansion of the loss function up to the second order to make <script type="math/tex">\alpha_t,g_t</script> independent from <script type="math/tex">f_{boost}^{t-1}</script></p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) \approx \sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) + L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>where <script type="math/tex">L^1=\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script> and <script type="math/tex">L^2=\frac{\partial L(y,\hat{y})}{\partial \hat{y}^2}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script>. </p>
<p>As observed the first term is a constant, we can remove it.</p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) =\sum_i^{n} L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>One important advantage of this approximation is that the value of the objective function only depends on $L^1$ and $L^2$. This is how XGBoost <strong>supports custom loss functions</strong>. We can optimize every loss function, including logistic regression and pairwise ranking, using exactly the same solver that takes $L^1$ and $L^2$ as input!</p>
<h3 id="Gradient-Tree-Boosting"><a href="#Gradient-Tree-Boosting" class="headerlink" title="Gradient Tree Boosting"></a>Gradient Tree Boosting</h3><p>Given a known partitions ${R_j^t,\ j=1,\dots,J_t}$ of a CART $g_t$, take $g_t$ and $\Omega(g_t)$ in loss function above, we have</p>
<script type="math/tex; mode=display">\mathbf{L}(b_j^t|\mathbf{D},R_j^t) =\sum_{j=1}^{J_t}( b_j^t \sum_{\mathbf{x}_i \in R_j^t} L^1 +  (b_j^t)^2 \sum_{\mathbf{x}_i \in R_j^t}\frac{1}{2}L^2 + (b_j^t)^2\frac{1}{2}\lambda) + \eta J</script><p>This is a quadratic form with respect to $b_j^t$, and we can compute the optimal<br>weight $b_j^t$ of region $j$ in $g_t$</p>
<script type="math/tex; mode=display">b_j^t = -\frac{\sum_{\mathbf{x}_i \in R_j^t} L^1}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2}</script><p>Once we know the partition, we can always find the best weights by the formula above. If we take it back into the loss function, we can have</p>
<script type="math/tex; mode=display">\mathbf{L}(R_j^t|\mathbf{D}) = -\frac{1}{2} \frac{(\sum_{\mathbf{x}_i \in R_j^t} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2} +\eta J_t</script><p>This is a function only replies on partition. This function(score) is like the impurity score for evaluating decision trees, except that <strong>it is derived for a wider range of objective functions (as long as we can have first and second order derivative)</strong>. </p>
<p>Now let`s go talk about the partition <script type="math/tex">\{R_j^t,\ j=1,\dots,J_t\}</script>. Similar to information gain rule in CART, <strong>we can use this score as impurity i.e. indicator for greedy approach to determine a split on region</strong> (leaf split). </p>
<p>For binary tree, assume that <script type="math/tex">R_{left}</script> and <script type="math/tex">R_{right}</script> are the instance sets of left and right nodes after the split <script type="math/tex">R_{parent}= R_{left}\cup R_{right}</script>. Then the loss reduction after the split is given by</p>
<script type="math/tex; mode=display">\mathbf{L}_{split}=  \frac{1}{2} [ \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} + \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} -  \frac{(\sum_{\mathbf{x}_i \in R_{parent}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{parent}}L^2}] - \eta</script><p>This also makes XGboost different from GBDT, <strong>not only the leaf of weight but also the partition of regions (leaves) are relevant to loss function</strong>.</p>
<img src="/2019/12/23/Ensemble/xgb.jpg" class="" width="500">
<h2 id="Adaptive-Boosting-Adaboost"><a href="#Adaptive-Boosting-Adaboost" class="headerlink" title="Adaptive Boosting (Adaboost)"></a>Adaptive Boosting (Adaboost)</h2><p>Adaboost was initially proposed for classification. In binary classification task where $y_i \in \lbrace -1,1 \rbrace$ and $\hat{y}=sign(g(\mathbf{x}_i))$, $g:\mathcal{X} \to [-1,1] $ . It sets loss function as</p>
<script type="math/tex; mode=display">L(y,\hat{y})=\exp(-y\hat{y})</script><p>which means higher loss for mis-classification and versus (only valid for binary case). For each iteration training k-th learner, the empirical loss function is</p>
<script type="math/tex; mode=display">\begin{aligned}
\mathbf{L}(\alpha_t,g_t|\mathbf{D})=&\sum_{i=1}^{n}\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_t g_t(\mathbf{x}_i)))\\
=&\sum_{i=1}^{n}w_i^t\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\\
=&\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t +\exp(\alpha_t)\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>where <script type="math/tex">w_i^t=\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i))</script>, which measures the performance of first $t-1$ ensemble on instance $i$. </p>
<h3 id="Calculate-alpha-t"><a href="#Calculate-alpha-t" class="headerlink" title="Calculate $\alpha_t$"></a>Calculate $\alpha_t$</h3><p>As observed, the effect of <script type="math/tex">\alpha_t</script> and <script type="math/tex">f_{boost}^{t-1}</script> to loss function is independent. Take the first order derivative we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial \mathbf{L}(\alpha_t,g_t|\mathbf{D})}{\partial \alpha_t}
=&-\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t + \exp(\alpha_t) \sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>Set <script type="math/tex">\frac{\partial \mathbf{L}(y,g_t)}{\partial \alpha_t}=0</script> we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\alpha_t
&=\frac{1}{2}\log(\frac{\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t}{\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t})\\
&=\frac{1}{2}\log(\frac{1-e_t}{e_t})
\end{aligned}</script><p>where $e_t$ is the <strong>weighted training error rate</strong> for $g_t$.</p>
<script type="math/tex; mode=display">e_t=\frac{\sum_{i=1}^{n}w_i^t\mathbf{1}(y_i=g_t(\mathbf{x}_i))}{\sum_{i=1}^{n}w_i^t}</script><p>Hense, if we know <script type="math/tex">g_t</script> we can get the <strong>closed form</strong> of optimal <script type="math/tex">\alpha_t</script>  directly and discard line searching. </p>
<h3 id="Weight-Update"><a href="#Weight-Update" class="headerlink" title="Weight Update"></a>Weight Update</h3><p>Once we add update the ensemble, we should update the weight for the next learner <script type="math/tex">g_{t+1}</script> as well. And as the formula shows, it only matters with the last update and the newly added learner performance which can save computation in practice.</p>
<script type="math/tex; mode=display">\begin{aligned}
f_{Adaboost}^{t}&=f_{Adaboost}^{t-1} + \alpha_tg_t(\mathbf{x})\\
w_i^{t+1}&=\exp(-y_if_{Adaboost}^{t})\\
&=\exp(-y_i(f_{Adaboost}^{t-1} + \alpha_tg_t))\\
&=w_i^{t}\times\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\end{aligned}</script><h3 id="Train-g-k"><a href="#Train-g-k" class="headerlink" title="Train $g_k$"></a>Train $g_k$</h3><p>In practice, the weak learner may be an algorithm that can use the weights $w_i^t$ on the training examples. Alternatively, when this is not possible, a subset of the training examples can be sampled according to $w_i^t$, and these (unweighted) resampled examples can be used to train the weak learner. [<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi0nvyBxd7mAhWNIjQIHeGdD9AQFjAAegQIBhAI&amp;url=https%3A%2F%2Fcseweb.ucsd.edu%2F~yfreund%2Fpapers%2FIntroToBoosting.pdf&amp;usg=AOvVaw21X5ZIA1LN2tzNBYDnISDR" target="_blank" rel="noopener">reference</a>]</p>
<p>Algorithms that can use weights:???</p>
<p>Compared with all previous algorithms, one special thing is we <strong>change the weight of training sample</strong> at each iteration of training weak leaner. In general, we assign more wight to samples mis-classified by last weak learner and versus. (<del>Q: Why weighting samples works?</del>A: cuz it was derived and reflected in loss function.)</p>
<img src="/2019/12/23/Ensemble/ada.jpg" class="" width="500">
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li><p>for t in 1 to K:<br> 2.1 train the learner $g_t$ with sample weighted by <script type="math/tex">w_i^t</script><br> 2.2 </p>
<ul>
<li>get the weighted training error rate $e_t$ (<del>mis-classification rate</del>) </li>
<li>calculate the weight for this learner <script type="math/tex; mode=display">\alpha_t=\frac{1}{2}\log(\frac{1-e_t}{e_t})</script></li>
</ul>
<p>2.3 update sample weight</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\left\{
\begin{aligned}
w_i^t e^{\alpha_i \times -1}, &\ if\ g_t(\mathbf{x}_i)=y_i\\
w_i^t e^{\alpha_i \times 1}, &\ else\\
\end{aligned}
\right.</script><p> if $\mathbf{x}_i$ is correctly classified by $g_t$ the weight should be shrunk by $e^{\alpha_i \times -1}$ otherwise we should emphasize this sample by multiplying $e^{\alpha_i \times 1}$ , else $w_i^t=w_i^{t-1}e^{\alpha_i \times 1}$<br> and then normalize the weight <script type="math/tex">w_i^{t+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script> </p>
</li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(X)</script></li>
</ol>
<h3 id="Regression-R2"><a href="#Regression-R2" class="headerlink" title="Regression (R2)"></a>Regression (R2)</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li>for t in 1 to K:<br> 2.1 train the learner <script type="math/tex">g_t</script> sample weighted by <script type="math/tex">w_i^t</script> and<br> 2.2 <ul>
<li>get the residuals <script type="math/tex">r_i^t = g_t((\mathbf{x}_i))-y_i</script> and the maximum abs value <script type="math/tex">r_{max}</script> </li>
<li>calculate the relative error <script type="math/tex">R_i^k = r_i^t/r_{max}</script> or <script type="math/tex">(r_i^t/r_{max})^2</script> or <script type="math/tex">1- \exp(- r_i^t/r_{max})</script> and take the weighted average as error rate <script type="math/tex; mode=display">e_t=\sum_i w_i^tR_i^t</script></li>
<li>Then calculate the weight<script type="math/tex; mode=display">\alpha_t=e_t/(1-e_t)</script>2.3 Update weights $w_i^{t+1}=w_i^k(\alpha_t)^{1-e_i^t}$ and normalize the weight  <script type="math/tex; mode=display">w_i^{k+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script></li>
</ul>
</li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(\mathbf{x})</script></li>
</ol>
<p>Basically the process of training classification or regression model is same (at each iteration $t$, train $g_t$, calculate error rate $e_t$, calculate weight $\alpha_t$ for $g_t$ and update sample weights according to $g_t$). The only difference lies in how to calculate the error rate and weights. </p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>In order to avoid over-fitting, Adaboost also introduced regularization term, which is a shrinkage multiplier on base leaner weight</p>
<script type="math/tex; mode=display">\alpha_t=v\alpha_t</script><p>where $v\in (0,1)$. This shrinkage multiplier $v$ is also called learning rate.</p>
<h2 id="Boosting-Tree-Summary"><a href="#Boosting-Tree-Summary" class="headerlink" title="Boosting Tree Summary"></a>Boosting Tree Summary</h2><p>Briefly speaking, there are three types of boosting tree introduced above. All three use greedy approach to train a sequence of decision trees.</p>
<ul>
<li>GBDT(SGBDT)<ul>
<li>first order Taylor approximation on loss function</li>
<li>training with <strong>pseudo-residuals from predecessor</strong></li>
<li>training leaf weight according to loss function but requires <strong>line search</strong></li>
<li>training partition independently</li>
<li>introduced stochastic sampling as regularization</li>
</ul>
</li>
<li>XGboost<ul>
<li>A second order Taylor approximation is imposed on loss function</li>
<li>training with original data</li>
<li><strong>calculating </strong>leaf weight with a closed form (relevant to loss function)</li>
<li>training partition is dependent on loss function (first and second order derivative)</li>
<li>flexibility on regularization</li>
</ul>
</li>
<li>Adaboost<ul>
<li>fixed loss function that only makes sense in binary classification</li>
<li>training with <strong>weighted</strong> data</li>
<li>training leaf weight <strong>semi</strong> according to loss function ($\alpha_k^t$ is relevant to loss function)</li>
<li>training partition independently</li>
<li>introduced shrinkage parameter (learning rate) as regularization</li>
</ul>
</li>
</ul>
<p>In summary, only XGboost contains no independent training step. Independence of GBDT lies in partition while Adaboost also has independent leaves weight training. More flexibility on regularization can be achieved in XGboost as they emphasized in their paper (most boosting methods did not pay enough attention on regularization)</p>
<h2 id="LightBGM"><a href="#LightBGM" class="headerlink" title="LightBGM"></a>LightBGM</h2><h2 id="Catboost"><a href="#Catboost" class="headerlink" title="Catboost"></a>Catboost</h2>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-learning/" rel="tag"># Machine learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/11/28/Entropy/" rel="next" title="Gini Index vs Entropy">
                  <i class="fa fa-chevron-left"></i> Gini Index vs Entropy
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2019/12/25/Hyperparameter/" rel="prev" title="Hyperparameter tuning">
                  Hyperparameter tuning <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Notation"><span class="nav-number">1.</span> <span class="nav-text">Notation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hard-amp-soft-voting"><span class="nav-number">2.</span> <span class="nav-text">Hard &amp; soft voting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bagging"><span class="nav-number">3.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#OOB-score"><span class="nav-number">3.1.</span> <span class="nav-text">OOB score??</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Random-forest"><span class="nav-number">3.2.</span> <span class="nav-text">Random forest</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Boosting"><span class="nav-number">4.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Boosting"><span class="nav-number">4.1.</span> <span class="nav-text">Gradient Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Boosting-Decision-Tree-GBDT"><span class="nav-number">4.1.1.</span> <span class="nav-text">Gradient Boosting Decision Tree (GBDT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-Gradient-Boosting-SGB"><span class="nav-number">4.1.2.</span> <span class="nav-text">Stochastic Gradient Boosting (SGB)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGboost"><span class="nav-number">4.2.</span> <span class="nav-text">XGboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization-on-Model-Complexity"><span class="nav-number">4.2.1.</span> <span class="nav-text">Regularization on Model Complexity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tayler-expansion-approximation"><span class="nav-number">4.2.2.</span> <span class="nav-text">Tayler expansion approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Tree-Boosting"><span class="nav-number">4.2.3.</span> <span class="nav-text">Gradient Tree Boosting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Boosting-Adaboost"><span class="nav-number">4.3.</span> <span class="nav-text">Adaptive Boosting (Adaboost)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Calculate-alpha-t"><span class="nav-number">4.3.1.</span> <span class="nav-text">Calculate $\alpha_t$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weight-Update"><span class="nav-number">4.3.2.</span> <span class="nav-text">Weight Update</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-g-k"><span class="nav-number">4.3.3.</span> <span class="nav-text">Train $g_k$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification"><span class="nav-number">4.3.4.</span> <span class="nav-text">Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-R2"><span class="nav-number">4.3.5.</span> <span class="nav-text">Regression (R2)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">4.3.6.</span> <span class="nav-text">Regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting-Tree-Summary"><span class="nav-number">4.4.</span> <span class="nav-text">Boosting Tree Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LightBGM"><span class="nav-number">4.5.</span> <span class="nav-text">LightBGM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Catboost"><span class="nav-number">4.6.</span> <span class="nav-text">Catboost</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Jiaxin Zhang"
    src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiaxin Zhang</p>
  <div class="site-description" itemprop="description">Curious, Open-eyed, Humble, Prepared, Sensitive.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiaxin Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
