<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Jiaxin Zhang" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of resp">
<meta name="keywords" content="Python, R, SQL, SPSS, Machine Learning, NLP, Statistics, Data Analysis">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiaxin Zhang">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Jiaxin Zhang">
<meta property="og:description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of resp">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Jiaxin Zhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiaxin Zhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">MSc in Statistical Machine Learning @ UofA</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/JasonZhang0619" class="github-corner" title="GitHub" aria-label="GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/25/Hyperparameter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/25/Hyperparameter/" class="post-title-link" itemprop="url">Hyperparameter tuning</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-25 21:49:08" itemprop="dateCreated datePublished" datetime="2019-12-25T21:49:08-07:00">2019-12-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-27 23:48:33" itemprop="dateModified" datetime="2019-12-27T23:48:33-07:00">2019-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/23/Ensenmble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/23/Ensenmble/" class="post-title-link" itemprop="url">Ensenmble learning</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-23 22:22:08" itemprop="dateCreated datePublished" datetime="2019-12-23T22:22:08-07:00">2019-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-30 18:26:18" itemprop="dateModified" datetime="2019-12-30T18:26:18-07:00">2019-12-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In statistics and machine learning, <a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener">Ensemble method</a> use <strong>multiple learning algorithms</strong> to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.<br>This post is mainly about the theory behind these big names. </p>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><ul>
<li>data<br>$\mathcal{X}$ input space ($p dimensions$) $\mathcal{Y}$ output space<br>$\mathbf{x}$ input variable $\mathbf{x} \in \mathbf{X}$<br>$n$ train sample size<br>$\mathbf{x}_i, y_i$ training input and output, $i= 1, \dots, n$<br>$w_i$ weight for sample $\mathbf{x}_i, y_i$<br>$\mathbf{D}=(\mathbf{X},\mathbf{y})$ is the design matrix $(\mathbf{x}_1,\dots, \mathbf{x}_n)^T$ and output vector $(y_1,\dots, y_n)^T$<br>$\mathcal{RC}(D)$ random choice from dataset $D$ <strong>with no replacement</strong><br>$\mathcal{RS}(D)$ random sampling from dataset $D$ <strong>with replacement</strong></li>
<li>learners<br>$f: \mathcal{X} \to \mathcal{Y}$ arbitrary function<br>$\mathcal{H}$ base learner space<br>$g: \mathcal{X} \to \mathcal{Y}$ and $g \in \mathcal{H}$ a base learner.<br>$g(\mathbf{x}|\mathbf{D})$ a base learner trained with \mathbf{D}<br>$K$ number of base learners<br>$\alpha_k$ weight for $g_k$, $k=1,\dots, K$</li>
<li>ensemble<br>$f_{*}^t: \mathcal{X} \to \mathcal{Y}$ The * ensemble with first $t$ base learners</li>
<li>loss function<br>$L(y,\hat{y})$ loss function comparing $y,\hat{y}$<br>$\mathbf{L}(*|\mathbf{D})$ loss target for * given $\mathbf{D}$<br>$\Omega(f)$ regularization function on $f$</li>
<li>CART<br>$R_j^t$ the j-th region partitioned by CART $g_t$<br>$J_t$ the total number of partitions  by CART $g_t$</li>
</ul>
<img src="/2019/12/23/Ensenmble/Notation.jpg" class="" width="500" title="notation for figs">
<hr>
<p>Given training data $\mathbf{D}$, regression task aims to find a regression function </p>
<script type="math/tex; mode=display">y=g(\mathbf{x}|\mathbf{D})</script><p>when $\mathcal{Y} = \mathbf{R}$. Similarly, classification results in a multivariate function that estimate the probabilities (or equivalent) of a given input belonging to every class when $\mathcal{Y}$ is a finite set of categories and returns the category with largest prob.</p>
<script type="math/tex; mode=display">y=\arg max_y P(\mathbf{x}|y,\mathbf{D}) = which\ max \mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>, $\mathcal{P}(\mathbf{x}|\mathbf{D})=(P(\mathbf{x}|1,\mathbf{D}),\dots, P(\mathbf{x}||\mathcal{Y}|,\mathbf{D}))^T$ denoted as $g(\mathbf{x}|\mathbf{D})$ which returns the vector of probs for all categories $1,\dots,|\mathcal{Y}|$. Hense, classification task can also be taken as multi-variate linear regression where the response variable is $\mathcal{P}(\mathbf{x}|\mathbf{D})$.</p>
<p>Either for regression or classification, we build multiple models and these models (weak tho) are called base learner. Ensemble learning integrated these base learner by taking a linear additive combination of $g_k(\mathbf{x}|D_k)$.</p>
<script type="math/tex; mode=display">f_{ensemble}=\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}|D_k)</script><img src="/2019/12/23/Ensenmble/Ensemble.jpg" class="" width="400" title="ensemble">
<h1 id="Hard-amp-soft-voting"><a href="#Hard-amp-soft-voting" class="headerlink" title="Hard &amp; soft voting"></a>Hard &amp; soft voting</h1><p>Voting is an idea specific for classification. Every base learner is trained on the <strong>whole dataset</strong> $D_k=\mathbf{D}$ and averaged with <strong>equal weight</strong> $\alpha_k=1/k$.</p>
<script type="math/tex; mode=display">f_{voting}=\sum_{k=1}^{K}\frac{1}{K}g_k(\mathbf{x}|\mathbf{D})=mean(g_k(\mathbf{x}|\mathbf{D}))</script><p>The soft voting is directly taking the average of prob vectors</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>while the hard voting instead takes the average of one-hot vector transformed from probs vectors.</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathbf{1}(\mathcal{P}(\mathbf{x}|\mathbf{D})==\max(\mathcal{P}(\mathbf{x}|\mathbf{D}))</script><p>For example, in one binary classification task we ensemble three classifiers, the prob that an observation belongs to class 1 equal (0.45, 0.45, 0.9). With soft voting the average prob is (0.45 + 0.45 + 0.9)/3 = 0.6 while hard voting will return the average prob equal (0 + 0 + 1)/3 = 0.33.</p>
<h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bootstrap aggregating, often abbreviated as bagging, have each base learner in the ensemble vote with <strong>equal weight</strong> $\alpha_k=1/k$. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn <strong>subset of the training set</strong> $D_k=\mathcal{RS}(\mathbf{D})$. (In statistics, bootstrapping is any test or metric that relies on <strong>random sampling with replacement</strong>.)</p>
<script type="math/tex; mode=display">f_{bagging}=mean(g_k(\mathbf{x}|\mathcal{RS}(\mathbf{D})))</script><h2 id="OOB-score"><a href="#OOB-score" class="headerlink" title="OOB score??"></a>OOB score??</h2><img src="/2019/12/23/Ensenmble/bagging.jpg" class="" width="400" title="bagging">
<h2 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h2><p>Random forest consists of more randomness. With <strong>base learner space $\mathcal{H}$ being CART</strong>(classification and regression tree), not only is <strong>training set randomly sampled</strong> piece by piece, but also a <strong>features subset is selected randomly</strong> </p>
<script type="math/tex; mode=display">D_k=\mathcal{RS}((\mathbf{X}_{:,\mathcal{RC}(1\dots,p)},\mathbf{y}))</script><img src="/2019/12/23/Ensenmble/rf.jpg" class="" width="400" title="random forrest">
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Boosting involves <strong>incrementally</strong> building an ensemble by training each new model instance to <strong><del>emphasize</del> correct the training instances mis-classified by previous models</strong>. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data.<br>Target is to reduce the Loss function</p>
<script type="math/tex; mode=display">\mathbf{L}(f|\mathbf{D}) = \sum_i^{n}L(y_i,f(\mathbf{x}_i))</script><script type="math/tex; mode=display">f_{boost}=\arg \min \mathbf{L}(\mathbf{\alpha},\mathbf{g}|\mathbf{D}) = \arg \min_{\mathbf{\alpha},\mathbf{g}}\sum_i^{n}L(y_i,\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}_i))</script><p>It is computationally impossible to optimize the target function with $K$ learners <script type="math/tex">g_k</script> and weights $\alpha_k$ at the same time. Hence, we take the idea of <strong>greedy approach</strong> and optimize the target function <strong>sequentially</strong>. We start from training a weak learner $g_1$ and add more learners one by one to further reduce the loss function. </p>
<p>Given first $t-1$ learners ensemble optimized <script type="math/tex">f_{boost}^{t-1}=\sum_{k=1}^{t-1}\alpha_k g_k(X)</script>, the k-th learner can be obtained by optimizing:</p>
<script type="math/tex; mode=display">\min_{\alpha_t}\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i))</script><script type="math/tex; mode=display">\alpha_t, g_t = \arg \min_{\alpha, g} \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha g(\mathbf{x}_i))</script><h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><p>The idea of gradient boosting is to apply a <strong>steepest descent</strong> step to this minimization problem. For arbitrary <strong>differentiable loss function</strong> $L$, </p>
<script type="math/tex; mode=display">\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}} = f(y,f_{boost}^{t-1}(\mathbf{x})) \doteq r^t</script><p>This gradient value is called pseudo-residuals <script type="math/tex">r^t</script> and it can be calculated for every sample $r_i^t$. In regression task, when loss function is MSE, pseudo-residual is actually real residual.</p>
<script type="math/tex; mode=display">L(y,g)=(y-g)^2 \\ \frac{\partial L(y,g)}{\partial g} = -2(y-g)</script><p>In classification task,</p>
<script type="math/tex; mode=display">?</script><p>Let the t-th learner <script type="math/tex">g_t = - r_i^t</script>, adding $g_t$ to must reduce the loss function</p>
<script type="math/tex; mode=display">\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) \ge \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)- \alpha_t r_i^t)</script><p>Ideally we wish to find a learner such that <script type="math/tex">g_t = - r_i^t</script> is true every where, but this is even not possible for all observed samples or worthwhile since this is just a intermediate step during iterations. However, we can still use pseudo-residuals to guide the training of <script type="math/tex">g_t</script>. More concretely, we take pseudo-residuals of <script type="math/tex">f_{boost}^{t-1}</script> as response variable.</p>
<script type="math/tex; mode=display">D_t=\{(\mathbf{x}_i, r_i^t)\}</script><p>After training $g_k$ we still need to determine a good weight $\alpha_k$ for it. It might be impossible to find a closed form of optimal solution but there are still ways to find one good enough. Line search is one the basic ways.</p>
<img src="/2019/12/23/Ensenmble/gb.jpg" class="" width="500"> 
<p>In general, Gradient Boosting includes two stages in each iteration: </p>
<ol>
<li>k-th learner <script type="math/tex">g_t</script> is trained by pseudo-residuals (training set <script type="math/tex">D_t=\{(\mathbf{x}_i, r_i^t)\}</script>) </li>
<li>Line search <script type="math/tex">\alpha_t = \arg \min_{\alpha} \sum_i^{n}(y_i,f_{GB}^{t-1}+\alpha g_t)</script></li>
</ol>
<h3 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting Decision Tree (GBDT)"></a>Gradient Boosting Decision Tree (GBDT)</h3><p>Take decision tree (CART) being weak learner space as example. For this special case, Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner. Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region. Using the indicator notation, the output of <script type="math/tex">g_t</script> for input <script type="math/tex">\mathbf{x}</script> can be written as the sum: </p>
<script type="math/tex; mode=display">g_t(\mathbf{x})=\sum_{j=1}^{J_t}b_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p>where $b_j^t$ is the value predicted in the region $R_j^t$. (The partitions for different tree $g_t$ should be distinct otherwise there is no potential to improve the boosting function.) Then Line search </p>
<script type="math/tex; mode=display">\alpha_t = \arg \min_{\alpha} \sum_i^n L(y_i,f_{GB}^{t-1}+\alpha g_t)</script><p>Friedman proposes to modify this algorithm so that it chooses a separate optimal value $ \alpha_j^t$ for each of the tree($g_t$)`s regions $R_j^t$, rather than a single $\alpha_t$ for the whole tree. He calls the modified algorithm “TreeBoost”. This is equivalent to </p>
<script type="math/tex; mode=display">\alpha = \arg \min_{\alpha} \sum_j^{J_t} \sum_{\mathbf{x}_i \in \mathbf{R}_j^t} L(y_i,f_{GB}^{t-1}+\alpha_j^t (\mathbf {1} _{R_j^t}(\mathbf{x}_i)))</script><h2 id="Stochastic-Gradient-Boosting-SGB"><a href="#Stochastic-Gradient-Boosting-SGB" class="headerlink" title="Stochastic Gradient Boosting (SGB)"></a>Stochastic Gradient Boosting (SGB)</h2><p>Friedman proposed a minor modification to the algorithm, motivated by bootstrap aggregation (“bagging”) method at each iteration of the algorithm, a base learner should be fit on a <strong>subsample of the training set</strong> drawn at random without replacement. It not only introduces randomness into the algorithm and help <strong>prevent overfitting</strong>, but also makes <strong>training faster</strong>, because regression trees have to be fit to smaller datasets at each iteration. He obtained that 0.5 ≤ f ≤ 0.8  leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.</p>
<h2 id="XGboost"><a href="#XGboost" class="headerlink" title="XGboost"></a>XGboost</h2><p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener">XGBoost</a> stands for Extreme Gradient Boosting; it is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best <strong>tree </strong>model. It employs a number of nifty tricks that make it exceptionally successful, particularly with structured data. </p>
<ul>
<li>computing second-order gradients, i.e. second partial derivatives of the loss function (similar to Newton’s method), which provides more information about the direction of gradients and how to get to the minimum of our loss function.</li>
<li>advanced regularization (L1 &amp; L2), which improves model generalization.</li>
<li>training is very fast and can be parallelized / distributed across clusters.</li>
</ul>
<h1 id="Regularization-on-Model-Complexity"><a href="#Regularization-on-Model-Complexity" class="headerlink" title="Regularization on Model Complexity"></a>Regularization on Model Complexity</h1><p>Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region</p>
<script type="math/tex; mode=display">g(\mathbf{x})=\sum_{j=1}^{J}b_j\mathbf {1} _{R_j}(\mathbf{x})</script><p>In XGBoost, we define the complexity as</p>
<script type="math/tex; mode=display">\Omega(g)=\eta J+\frac{1}{2}\lambda \sum_{j=1}^{J}b_j^2</script><p>First term $\eta J$ is the penalty on the number of leaves $J$, which is one source of complexity, and $\eta$ is a hyper-parameter to control the strength of this penalty. Complexity also comes from $b_j$ and the second term is the $L_2$ regularization on $b_j$, which is quite similar to Ridge regression. </p>
<p>Of course, there is more than one way to define the complexity, but this one works well in practice. The regularization is one part most tree packages treat less carefully, or simply ignore. This was because the traditional treatment of tree learning only emphasized improving impurity, while the complexity control was left to heuristics. </p>
<p>The objective function for boosting at each iteration can be modified as below</p>
<script type="math/tex; mode=display">\min_{\alpha_t, g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i)) + \Omega(\alpha_tg_t)</script><p>A CART $g_t$ weighted by $\alpha_t$ is still a CART,</p>
<script type="math/tex; mode=display">\alpha_tg_t(\mathbf{x})=\sum_{j=1}^{J}\alpha_tb_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p><strong>Since in following Steps, we do not train a CART by information gain rule or any principle independent from loss function, we can cancel the weight $\alpha_t$</strong></p>
<script type="math/tex; mode=display">\min_{g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+g_t(\mathbf{x}_i)) + \Omega(g_t)</script><h1 id="Tayler-expansion-approximation"><a href="#Tayler-expansion-approximation" class="headerlink" title="Tayler expansion approximation"></a>Tayler expansion approximation</h1><p>One key specialty of XGboost is that it approximates this loss function by taking Taylor expansion of the loss function up to the second order to make <script type="math/tex">\alpha_t,g_t</script> independent from <script type="math/tex">f_{boost}^{t-1}</script></p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) \approx \sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) + L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>where <script type="math/tex">L^1=\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script> and <script type="math/tex">L^2=\frac{\partial L(y,\hat{y})}{\partial \hat{y}^2}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script>. </p>
<p>As observed the first term is a constant, we can remove it.</p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) =\sum_i^{n} L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>One important advantage of this approximation is that the value of the objective function only depends on $L^1$ and $L^2$. This is how XGBoost <strong>supports custom loss functions</strong>. We can optimize every loss function, including logistic regression and pairwise ranking, using exactly the same solver that takes $L^1$ and $L^2$ as input!</p>
<h1 id="Gradient-Tree-Boosting"><a href="#Gradient-Tree-Boosting" class="headerlink" title="Gradient Tree Boosting"></a>Gradient Tree Boosting</h1><p>Given a known partitions ${R_j^t,\ j=1,\dots,J_t}$ of a CART $g_t$, take $g_t$ and $\Omega(g_t)$ in loss function above, we have</p>
<script type="math/tex; mode=display">\mathbf{L}(b_j^t|\mathbf{D},R_j^t) =\sum_{j=1}^{J_t}( b_j^t \sum_{\mathbf{x}_i \in R_j^t} L^1 +  (b_j^t)^2 \sum_{\mathbf{x}_i \in R_j^t}\frac{1}{2}L^2 + (b_j^t)^2\frac{1}{2}\lambda) + \eta J</script><p>This is a quadratic form with respect to $b_j^t$, and we can compute the optimal<br>weight $b_j^t$ of region $j$ in $g_t$</p>
<script type="math/tex; mode=display">b_j^t = -\frac{\sum_{\mathbf{x}_i \in R_j^t} L^1}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2}</script><p>Once we know the partition, we can always find the best weights by the formula above. If we take it back into the loss function, we can have</p>
<script type="math/tex; mode=display">\mathbf{L}(R_j^t|\mathbf{D}) = -\frac{1}{2} \frac{(\sum_{\mathbf{x}_i \in R_j^t} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2} +\eta J_t</script><p>This is a function only replies on partition. This function(score) is like the impurity<br>score for evaluating decision trees, except that <strong>it is derived for a wider range of objective functions (as long as we can have first and second order derivative)</strong>. </p>
<p>Now let`s go talk about the partition <script type="math/tex">\{R_j^t,\ j=1,\dots,J_t\}</script>. Similar to information gain rule in CART, we can use this score as impurity i.e. indicator for greedy approach to determine a split on region (leaf split). </p>
<p>For binary tree, assume that <script type="math/tex">R_{left}</script> and <script type="math/tex">R_{right}</script> are the instance sets of left and right nodes after the split <script type="math/tex">R_{parent}= R_{left}\cup R_{right}</script>. Then the loss reduction after the split is given by</p>
<script type="math/tex; mode=display">\mathbf{L}_{split}=  \frac{1}{2} [ \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} + \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} -  \frac{(\sum_{\mathbf{x}_i \in R_{parent}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{parent}}L^2}] - \eta</script><img src="/2019/12/23/Ensenmble/xgb.jpg" class="" width="500">
<h2 id="Adaptive-Boosting-Adaboost"><a href="#Adaptive-Boosting-Adaboost" class="headerlink" title="Adaptive Boosting (Adaboost)"></a>Adaptive Boosting (Adaboost)</h2><p>Adaboost was initially proposed for classification. In binary classification task where $y_i \in \lbrace -1,1 \rbrace$ and $\hat{y}=sign(g(\mathbf{x}_i))$, $g:\mathcal{X} \to [-1,1] $ . It sets loss function as</p>
<script type="math/tex; mode=display">L(y,\hat{y})=\exp(-y\hat{y})</script><p>which means higher loss for mis-classification and versus (only valid for binary case). For each iteration training k-th learner, the empirical loss function is</p>
<script type="math/tex; mode=display">\begin{aligned}
\mathbf{L}(\alpha_t,g_t|\mathbf{D})=&\sum_{i=1}^{n}\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_t g_t(\mathbf{x}_i)))\\
=&\sum_{i=1}^{n}w_i^t\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\\
=&\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t +\exp(\alpha_t)\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>where <script type="math/tex">w_i^t=\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i))</script>, which measures the performance of first $t-1$ ensemble on instance $i$. </p>
<h3 id="Calculate-alpha-t"><a href="#Calculate-alpha-t" class="headerlink" title="Calculate $\alpha_t$"></a>Calculate $\alpha_t$</h3><p>As observed, the effect of <script type="math/tex">\alpha_t</script> and <script type="math/tex">f_{boost}^{t-1}</script> to loss function is independent. Take the first order derivative we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial \mathbf{L}(\alpha_t,g_t|\mathbf{D})}{\partial \alpha_t}
=&-\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t + \exp(\alpha_t) \sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>Set <script type="math/tex">\frac{\partial \mathbf{L}(y,g_t)}{\partial \alpha_t}=0</script> we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\alpha_t
&=\frac{1}{2}\log(\frac{\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t}{\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t})\\
&=\frac{1}{2}\log(\frac{1-e_t}{e_t})
\end{aligned}</script><p>where $e_t$ is the <strong>weighted training error rate</strong> for $g_t$.</p>
<script type="math/tex; mode=display">e_t=\frac{\sum_{i=1}^{n}w_i^t\mathbf{1}(y_i=g_t(\mathbf{x}_i))}{\sum_{i=1}^{n}w_i^t}</script><p>Hense, if we know <script type="math/tex">g_t</script> we can get the <strong>closed form</strong> of optimal <script type="math/tex">\alpha_t</script>  directly and discard line searching. </p>
<h3 id="Weight-Update"><a href="#Weight-Update" class="headerlink" title="Weight Update"></a>Weight Update</h3><p>Once we add update the ensemble, we should update the weight for the next learner <script type="math/tex">g_{t+1}</script> as well. And as the formula shows, it only matters with the last update and the newly added learner performance which can save computation in practice.</p>
<script type="math/tex; mode=display">\begin{aligned}
f_{Adaboost}^{t}&=f_{Adaboost}^{t-1} + \alpha_tg_t(\mathbf{x})\\
w_i^{t+1}&=\exp(-y_if_{Adaboost}^{t})\\
&=\exp(-y_i(f_{Adaboost}^{t-1} + \alpha_tg_t))\\
&=w_i^{t}\times\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\end{aligned}</script><h3 id="Train-g-k"><a href="#Train-g-k" class="headerlink" title="Train $g_k$"></a>Train $g_k$</h3><p>In practice, the weak learner may be an algorithm that can use the weights $w_i^t$ on the training examples. Alternatively, when this is not possible, a subset of the training examples can be sampled according to $w_i^t$, and these (unweighted) resampled examples can be used to train the weak learner. [<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi0nvyBxd7mAhWNIjQIHeGdD9AQFjAAegQIBhAI&amp;url=https%3A%2F%2Fcseweb.ucsd.edu%2F~yfreund%2Fpapers%2FIntroToBoosting.pdf&amp;usg=AOvVaw21X5ZIA1LN2tzNBYDnISDR" target="_blank" rel="noopener">reference</a>]</p>
<p>Algorithms that can use weights:???</p>
<p>Compared with all previous algorithms, one special thing is we <strong>change the weight of training sample</strong> at each iteration of training weak leaner. In general, we assign more wight to samples mis-classified by last weak learner and versus. (<del>Q: Why weighting samples works?</del>A: cuz it was derived and reflected in loss function.)</p>
<img src="/2019/12/23/Ensenmble/ada.jpg" class="" width="500">
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li><p>for t in 1 to K:<br> 2.1 train the learner $g_t$ with sample weighted by <script type="math/tex">w_i^t</script><br> 2.2 </p>
<ul>
<li>get the weighted training error rate $e_t$ (<del>mis-classification rate</del>) </li>
<li>calculate the weight for this learner <script type="math/tex; mode=display">\alpha_t=\frac{1}{2}\log(\frac{1-e_t}{e_t})</script></li>
</ul>
<p>2.3 update sample weight</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\left\{
\begin{aligned}
w_i^t e^{\alpha_i \times -1}, &\ if\ g_t(\mathbf{x}_i)=y_i\\
w_i^t e^{\alpha_i \times 1}, &\ else\\
\end{aligned}
\right.</script><p> if $\mathbf{x}_i$ is correctly classified by $g_t$ the weight should be shrunk by $e^{\alpha_i \times -1}$ otherwise we should emphasize this sample by multiplying $e^{\alpha_i \times 1}$ , else $w_i^t=w_i^{t-1}e^{\alpha_i \times 1}$<br> and then normalize the weight <script type="math/tex">w_i^{t+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script> </p>
</li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(X)</script></li>
</ol>
<h3 id="Regression-R2"><a href="#Regression-R2" class="headerlink" title="Regression (R2)"></a>Regression (R2)</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li><p>for t in 1 to K:<br> 2.1 train the learner <script type="math/tex">g_t</script> sample weighted by <script type="math/tex">w_i^t</script> and<br> 2.2 </p>
<ul>
<li>get the residuals <script type="math/tex">r_i^t = g_t((\mathbf{x}_i))-y_i</script> and the maximum abs value <script type="math/tex">r_{max}</script> </li>
<li>calculate the relative error <script type="math/tex">R_i^k = r_i^t/r_{max}</script> or <script type="math/tex">(r_i^t/r_{max})^2</script> or <script type="math/tex">1- \exp(- r_i^t/r_{max})</script> and take the weighted average as error rate <script type="math/tex; mode=display">e_t=\sum_i w_i^tR_i^t</script></li>
<li>Then calculate the weight<script type="math/tex; mode=display">\alpha_t=e_t/(1-e_t)</script></li>
</ul>
<p>2.3 Update weights $w_i^{t+1}=w_i^k(\alpha_t)^{1-e_i^t}$ and normalize the weight </p>
<script type="math/tex; mode=display">w_i^{k+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script></li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(\mathbf{x})</script></li>
</ol>
<p>Basically the process of training classification or regression model is same (at each iteration $t$, train $g_t$, calculate error rate $e_t$, calculate weight $\alpha_t$ for $g_t$ and update sample weights according to $g_t$). The only difference lies in how to calculate the error rate and weights. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/28/Entropy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/28/Entropy/" class="post-title-link" itemprop="url">Gini Index vs Entropy</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-28 19:34:18" itemprop="dateCreated datePublished" datetime="2019-11-28T19:34:18-07:00">2019-11-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-23 21:29:32" itemprop="dateModified" datetime="2019-12-23T21:29:32-07:00">2019-12-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Gini Index</strong> and <strong>Entropy</strong> are two types of measure of <strong>impurity $I$</strong> (dispersion, inequality or variance) of <strong>categorical</strong> distribution which is used in information-gain rules in decision tree. Given $k$ categories and corresponding probability $p_k$, <strong>Gini Index</strong> (also called <a href="https://en.wikipedia.org/wiki/Gini_coefficient" target="_blank" rel="noopener">Gini coefficient</a>) is defined as below :</p>
<script type="math/tex; mode=display">I_{Gini}(p) = \sum_{i=1}^{k}p_i(1-p_i) = 1-\sum_{i=1}^{k}p_i^2 \in (0,1-\frac{1}{k})</script><p>while <strong>Entropy</strong> is defined as below:</p>
<script type="math/tex; mode=display">I_{Entropy}(p) = -\sum_{i=1}^{k}p_i\log(p_i) \in (0,1)</script><h1 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h1><ul>
<li>formula</li>
<li>relationship with impurity: The more impurity the distribution is, the larger either measure is.</li>
</ul>
<h1 id="Difference"><a href="#Difference" class="headerlink" title="Difference"></a>Difference</h1><ul>
<li>computing efficiency: Entropy is more computationally heavy due to the log in the equation. </li>
<li>sensitivity to tiny prob: Entropy is more sensitive to small prob (<0.2) and less sensitive to large prob (>0.2), which means <strong>sensitivity to noise</strong> at the same time.</li>
</ul>
<h1 id="Information-gain-rule"><a href="#Information-gain-rule" class="headerlink" title="Information-gain rule"></a>Information-gain rule</h1><p>Decision tree consists of sequence of decision nodes. In decision tree training algorithm, we recursively determine the decision nodes with respect to </p>
<ul>
<li>which feature or variable we use</li>
<li>where the decision boundary(s) for children nodes, i.e. where to split</li>
</ul>
<p>Information-gain rule is the guide for determining the decision nodes. <strong>Information-gain</strong> is defined as the difference of information (impurity) between all children nodes and their parent node. In other words, <strong>it measures how much information the splitting carries</strong>. Given the parent node $a$ is split into several children nodes $c_i$ by a splitting $T$, the Information-gain is defined as</p>
<script type="math/tex; mode=display">IG(T,a) = I(p(a)) - \sum_{i}\frac{|c_i|}{|p|}I(p(c_i))</script><p>where $|a|$ means the number of samples in node $a$ and $p(a)$ means the distribution of node $a$.</p>
<p>The idea of <strong>Information-gain rule</strong> is greedy algorithm. For each node, without other constrains(max depth, nothing to split), we use the splitting that results in <strong>maximum information gain</strong> and we recursively repeat layer by layer, node by node until all nodes contains no information (impurity is 0). It might stop early if there is constrain on the number of layers (max depth). For regression tree where the target variable is continuous, the impurity is measured by <strong>MSE</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/03/SSC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/SSC/" class="post-title-link" itemprop="url">SSC</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2019-11-03 17:25:03 / Modified: 19:34:03" itemprop="dateCreated datePublished" datetime="2019-11-03T17:25:03-07:00">2019-11-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">"C:\\Users\\jason\\Desktop\\SSC2019CaseStudy"</span>)</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Explore-the-cell-images"><a href="#Explore-the-cell-images" class="headerlink" title="Explore the cell images"></a>Explore the cell images</h1><h2 id="load-train-dataset"><a href="#load-train-dataset" class="headerlink" title="load train dataset"></a>load train dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df=pd.read_csv(<span class="string">'train_label.csv'</span>,index_col=<span class="string">'image_name'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>image_name</th>
      <th>count</th>
      <th>blur</th>
      <th>stain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A01_C1_F1_s01_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s02_w1.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s02_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s03_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s04_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,n,i+<span class="number">1</span>)</span><br><span class="line">    img_name=rd.choice(df.index)</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name)</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(img_name)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_3_0.png" class="" title="This is an image">
<h2 id="compare-img-with-different-cell-count"><a href="#compare-img-with-different-cell-count" class="headerlink" title="compare img with different cell count"></a>compare img with different cell count</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">img_name=[<span class="string">'A01_C1_F1_s13_w1.TIF'</span>,</span><br><span class="line">   <span class="string">'A02_C5_F1_s09_w1.TIF'</span>,</span><br><span class="line">    <span class="string">'A16_C66_F1_s04_w1.TIF'</span>]</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_5_0.png" class="" title="This is an image">
<h2 id="compare-img-with-different-level-of-F-and-w"><a href="#compare-img-with-different-level-of-F-and-w" class="headerlink" title="compare img with different level of F and w"></a>compare img with different level of F and w</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img_name = df.filter(like=<span class="string">'C14'</span>,axis=<span class="number">0</span>).filter(like=<span class="string">'s11'</span>,axis=<span class="number">0</span>).index</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_7_0.png" class="" title="This is an image">
<h1 id="Linear-regression-with-grey-scale"><a href="#Linear-regression-with-grey-scale" class="headerlink" title="Linear regression with grey scale"></a>Linear regression with grey scale</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Regression <span class="keyword">import</span> PoolRegressor</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br></pre></td></tr></table></figure>
<h2 id="For-F1-w1-images"><a href="#For-F1-w1-images" class="headerlink" title="For F1 w1 images"></a>For F1 w1 images</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">F = <span class="string">'F1'</span></span><br><span class="line">w = <span class="string">'w1'</span></span><br><span class="line">X, df = utils.read_imgset(csv_path=<span class="string">'train_label.csv'</span>,train=<span class="literal">True</span>, F=F, w=w, hist = <span class="literal">True</span>)</span><br><span class="line">X, df = shuffle(X, df, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="cross-validation"><a href="#cross-validation" class="headerlink" title="cross validation"></a>cross validation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">kf = KFold(n_splits=<span class="number">10</span>)</span><br><span class="line">fit=PoolRegressor(pool= <span class="literal">False</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X[test,])</span><br><span class="line">    df[<span class="string">'pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  0.09604340485504105 +/- 0.006622262312150045
test mse =  2.389303273614496 +/- 0.33634267102092574
</code></pre><p>train mse is way smaller than test mse, <strong>overfitting alert</strong>!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_15_1.png" class="" title="This is an image">
<p>The error increase when more cells in the image</p>
<h2 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">img_name=[<span class="string">'A01_C1_F1_s13_w1.TIF'</span>,</span><br><span class="line">   <span class="string">'A02_C5_F1_s09_w1.TIF'</span>,</span><br><span class="line">    <span class="string">'A16_C66_F1_s04_w1.TIF'</span>]</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">3</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">15</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_18_0.png" class="" title="This is an image">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fit=PoolRegressor(window=<span class="number">15</span>,step=<span class="number">15</span>,pool= <span class="literal">True</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'pooling_pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X[test,])</span><br><span class="line">    df[<span class="string">'pooling_pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  0.5545320752842191 +/- 0.01830920026615093
test mse =  0.6205896336762233 +/- 0.17798416968612663
</code></pre><p>Overfitting issue is solved by pooling (setting wider bins)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'pooling_pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_21_1.png" class="" title="This is an image">
<p><strong>heteroscedasticity</strong> the residuals still get larger as the prediction moves from small to large</p>
<p>How to Fix</p>
<ul>
<li>The most frequently successful solution is to <strong>transform</strong> a variable.</li>
<li>Often heteroscedasticity indicates that a <strong>variable is missing</strong>.</li>
</ul>
<h3 id="log-trasforamtion-failed"><a href="#log-trasforamtion-failed" class="headerlink" title="log trasforamtion (failed)"></a>log trasforamtion (failed)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X_log=np.log(X+<span class="number">1</span>)</span><br><span class="line">fit=PoolRegressor(window=<span class="number">15</span>,step=<span class="number">15</span>,pool= <span class="literal">True</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'log_pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X_log[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X_log[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X_log[test,])</span><br><span class="line">    df[<span class="string">'log_pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  137.2764500289679 +/- 6.5020079871442205
test mse =  196.17602353002354 +/- 132.57460287167424
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'log_pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_25_1.png" class="" title="This is an image">
<p>Patterns like this indicate that a variable needs to be transformed you probably need to create a <strong>nonlinear</strong> model</p>
<h2 id="Topology-feature"><a href="#Topology-feature" class="headerlink" title="Topology feature"></a>Topology feature</h2><h2 id="overlapping-example"><a href="#overlapping-example" class="headerlink" title="overlapping example"></a>overlapping example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/hello-world/" class="post-title-link" itemprop="url">hello-world</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-03 16:49:19" itemprop="dateCreated datePublished" datetime="2019-11-03T16:49:19-07:00">2019-11-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Jiaxin Zhang"
    src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiaxin Zhang</p>
  <div class="site-description" itemprop="description">4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiaxin Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
