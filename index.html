<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Jiaxin Zhang" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of resp">
<meta name="keywords" content="Python, R, SQL, SPSS, Machine Learning, NLP, Statistics, Data Analysis">
<meta property="og:type" content="website">
<meta property="og:title" content="Jiaxin Zhang">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Jiaxin Zhang">
<meta property="og:description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of resp">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Jiaxin Zhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiaxin Zhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">MSc in Statistical Machine Learning @ UofA</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/JasonZhang0619" class="github-corner" title="GitHub" aria-label="GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/02/changba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/02/changba/" class="post-title-link" itemprop="url">Scraping Changba data with python + selenium</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-02 19:34:18" itemprop="dateCreated datePublished" datetime="2020-02-02T19:34:18-07:00">2020-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-03 15:22:15" itemprop="dateModified" datetime="2020-02-03T15:22:15-07:00">2020-02-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Firefox()</span><br></pre></td></tr></table></figure>
<h1 id="Scraping-all-work-urls-from-main-personal-page"><a href="#Scraping-all-work-urls-from-main-personal-page" class="headerlink" title="Scraping all work urls from main personal page"></a>Scraping all work urls from main personal page</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">URL = <span class="string">"http://changba.com/u/26936044"</span></span><br><span class="line">driver.get(URL)</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>: </span><br><span class="line">        driver.find_element_by_id(<span class="string">"loadWork"</span>).click()</span><br><span class="line">    <span class="keyword">except</span>: </span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#the main page only shows your most recent works which is only part of the whole list</span></span><br><span class="line"><span class="comment">#we need to keep clicking "加載更多" (load more) until we no more available</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># if we only want the name list:</span></span><br><span class="line">work_list=driver.find_element_by_id(<span class="string">'work_list'</span>)</span><br><span class="line">len(work_list.text.split(<span class="string">'\n'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>837
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get the list of name and corresponding url</span></span><br><span class="line">WorkUrl_list=[]</span><br><span class="line">works=driver.find_elements_by_class_name(<span class="string">'userPage-work-li'</span>) <span class="comment"># all work nodes belong to this class</span></span><br><span class="line"><span class="comment">#you can also use clearfix which actually works better since the last one does not belongs to this class</span></span><br><span class="line"><span class="keyword">for</span> work <span class="keyword">in</span> works:</span><br><span class="line">    <span class="keyword">try</span>: </span><br><span class="line">        child=work.find_element_by_xpath(<span class="string">".//*"</span>)</span><br><span class="line">	<span class="comment">#By this step i can find the only child (attribute node) with attributes &lt;a href=...&gt;&lt;/a&gt;</span></span><br><span class="line">    <span class="keyword">except</span>: </span><br><span class="line">        <span class="keyword">break</span> <span class="comment">#the last one is blank with no child</span></span><br><span class="line">    name=string.capwords(child.text.split(<span class="string">'【'</span>)[<span class="number">0</span>].lower())<span class="comment">#Name Format</span></span><br><span class="line">    url=child.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    WorkUrl_list.append((name,url)) <span class="comment"># save a list of tuples [(work name, url)]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(WorkUrl_list)</span><br></pre></td></tr></table></figure>
<pre><code>837
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#for work in works:</span></span><br><span class="line"><span class="comment">#    url=driver.find_element_by_link_text(work.text).get_attribute('href')</span></span><br><span class="line"><span class="comment">#    WorkUrl_list.append((work.text,url))</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_pages = pd.DataFrame(WorkUrl_list, columns = [<span class="string">'name'</span>, <span class="string">'Page'</span>])</span><br></pre></td></tr></table></figure>
<h1 id="Scraping-a-page-for-single-work"><a href="#Scraping-a-page-for-single-work" class="headerlink" title="Scraping a page for single work"></a>Scraping a page for single work</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_pages.loc[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<pre><code>name                                           Nasa
Page    http://changba.com/s/GZr_J5_BvVpGx_odmAqtBQ
Name: 3, dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#scrap some info about this work</span></span><br><span class="line">URL = pd_pages.loc[<span class="number">3</span>].Page <span class="comment"># get url</span></span><br><span class="line">driver.get(URL) <span class="comment"># get page </span></span><br><span class="line">audience=driver.find_element_by_class_name(<span class="string">'audience'</span>).text <span class="comment"># get information in audience node</span></span><br><span class="line">comment=driver.find_element_by_class_name(<span class="string">'comment'</span>).text</span><br><span class="line">presents=driver.find_element_by_class_name(<span class="string">'presents'</span>).text</span><br><span class="line">share=driver.find_element_by_class_name(<span class="string">'share'</span>).text</span><br></pre></td></tr></table></figure>
<p>Bad new is there is no time info available on the website (unlike phone). The only way we can estimate the submitting time is to use the earliest comment posting time. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#estimate submitting time</span></span><br><span class="line">times=driver.find_elements_by_class_name(<span class="string">'post-time'</span>) <span class="comment">#get post-time of comments</span></span><br><span class="line"><span class="keyword">if</span> len(times) &gt;<span class="number">0</span>: <span class="comment"># if there is comments</span></span><br><span class="line">    time=pd.Timestamp(times[<span class="number">-1</span>].text) <span class="comment"># take the earliest time</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#locate and scrap the url of this work</span></span><br><span class="line">html=str(requests.get(URL).content)</span><br><span class="line">start, end =re.search(<span class="string">r'http://upscuw.changba.com/\d*\.mp3'</span>,html).span() <span class="comment"># find mp3 file url in raw html</span></span><br><span class="line">song_url=html[start:end]</span><br></pre></td></tr></table></figure>
<h1 id="For-all-pages"><a href="#For-all-pages" class="headerlink" title="For all pages"></a>For all pages</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> NoSuchElementException</span><br><span class="line"><span class="comment">#it happens when the page is forbidden</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pd_urls=pd.DataFrame(<span class="literal">None</span>,</span><br><span class="line">                     columns = [<span class="string">'audience'</span>, <span class="string">'comment'</span>,<span class="string">'presents'</span>,<span class="string">'share'</span>,<span class="string">'url'</span>,<span class="string">'Time'</span>],</span><br><span class="line">                     index=pd_pages.index)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pd_urls.index:</span><br><span class="line">    URL = pd_pages.loc[i].Page</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.get(URL)</span><br><span class="line">        pd_urls.audience[i]=driver.find_element_by_class_name(<span class="string">'audience'</span>).text</span><br><span class="line">        pd_urls.comment[i]=driver.find_element_by_class_name(<span class="string">'comment'</span>).text</span><br><span class="line">        pd_urls.presents[i]=driver.find_element_by_class_name(<span class="string">'presents'</span>).text</span><br><span class="line">        pd_urls.share[i]=driver.find_element_by_class_name(<span class="string">'share'</span>).text</span><br><span class="line">        </span><br><span class="line">        times=driver.find_elements_by_class_name(<span class="string">'post-time'</span>) <span class="comment">#get post-time of comments</span></span><br><span class="line">        <span class="keyword">if</span> len(times) &gt;<span class="number">0</span>: <span class="comment"># if there is comments</span></span><br><span class="line">            pd_urls.Time[i]=pd.Timestamp(times[<span class="number">-1</span>].text) <span class="comment"># take the earliest time</span></span><br><span class="line">        </span><br><span class="line">        html=str(requests.get(URL).content)</span><br><span class="line">        start, end =re.search(<span class="string">r'a="https*://.*\.changba\.com.*/\d*\.mp3'</span>,html).span()</span><br><span class="line">        pd_urls.url[i]=html[start+<span class="number">3</span>:end] <span class="comment"># updated from previous one to be more robost.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> NoSuchElementException: <span class="comment"># the work is not permited to access due to sensitive words in name</span></span><br><span class="line">        print(<span class="string">'NoSuchElementException for index '</span>,i)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">except</span> AttributeError: <span class="comment"># no .mp3 file found in html (if there is a mv)</span></span><br><span class="line">        print(<span class="string">'AttributeError for index '</span>,i)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt: <span class="comment"># control to stop </span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    time.sleep(<span class="number">2</span>) <span class="comment"># avoid frequent request (previous steps are slow tho)</span></span><br></pre></td></tr></table></figure>
<pre><code>AttributeError for index  157
NoSuchElementException for index  171
NoSuchElementException for index  179
NoSuchElementException for index  233
NoSuchElementException for index  394
AttributeError for index  399
NoSuchElementException for index  469
NoSuchElementException for index  478
NoSuchElementException for index  484
AttributeError for index  540
AttributeError for index  541
NoSuchElementException for index  557
NoSuchElementException for index  736
NoSuchElementException for index  790
AttributeError for index  803
AttributeError for index  805
AttributeError for index  808
AttributeError for index  813
AttributeError for index  821
AttributeError for index  822
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.close()</span><br><span class="line">pd_urls.to_csv(<span class="string">'changba_urls.csv'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pd_merge=pd_pages.join(pd_urls)</span><br><span class="line">pd_merge</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>Page</th>
      <th>audience</th>
      <th>comment</th>
      <th>presents</th>
      <th>share</th>
      <th>url</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>忘我</td>
      <td>http://changba.com/s/EXb4wphbN-GY464jDRV_KA</td>
      <td>64</td>
      <td>4</td>
      <td>8</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/9...</td>
      <td>2019-12-29 00:00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real Friends</td>
      <td>http://changba.com/s/FBZMCWgrLY3sO7sUEFbgSg</td>
      <td>32</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>http://ksapuw.changba.com/userdata/userwork/81...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>When I Was Your Man</td>
      <td>http://changba.com/s/GZr_J5_BvVr4a-ftEiASmw</td>
      <td>29</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>http://upscuw.changba.com/1208653739.mp3</td>
      <td>2019-12-29 00:00:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Nasa</td>
      <td>http://changba.com/s/GZr_J5_BvVpGx_odmAqtBQ</td>
      <td>25</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>http://upscuw.changba.com/1208653711.mp3</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Scars To Your</td>
      <td>http://changba.com/s/oTPTKN_3qXjKpIfapWDAAA</td>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://qiniuuwmp3.changba.com/1208653567.mp3</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>832</th>
      <td>恒星流星</td>
      <td>http://changba.com/s/lxnKkg0S0RVrwOn6rjxZOg</td>
      <td>41</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/8...</td>
      <td>2013-08-05 00:00:00</td>
    </tr>
    <tr>
      <th>833</th>
      <td>也许明天</td>
      <td>http://changba.com/s/z1FbhFD6pZ669cvjWIcqfA</td>
      <td>106</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/8...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>834</th>
      <td>剪爱</td>
      <td>http://changba.com/s/TIO4hN908E8Wndka1ntQpQ</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/7...</td>
      <td>2013-08-01 00:00:00</td>
    </tr>
    <tr>
      <th>835</th>
      <td>空白格</td>
      <td>http://changba.com/s/co-YKmeP2LT5THi79AR9gQ</td>
      <td>72</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/5...</td>
      <td>2013-08-15 00:00:00</td>
    </tr>
    <tr>
      <th>836</th>
      <td>Listen</td>
      <td>http://changba.com/s/BCpW1QzqtJp1bFGVkMadXw</td>
      <td>63</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/4...</td>
      <td>2013-08-01 00:00:00</td>
    </tr>
  </tbody>
</table>
<p>837 rows × 8 columns</p>
</div>



<h2 id="One-interesting-question"><a href="#One-interesting-question" class="headerlink" title="One interesting question"></a>One interesting question</h2><p>Recall that we estimate the submitting time by earliest comment time, some work has <strong>no comment</strong> and some work are firstly commented <strong>later than submitted</strong>. Hence,the time stamps are collected with some <strong>missing time</strong> and some <strong>delayed time</strong> and we want to correct them such that our time stamps should be consistent in <strong>non-increasing order</strong>. How to correct this time series?</p>
<p>Denote the timesatamps as $t_i$ and our estimate as $\hat{t}_i$, what we are sure about is:</p>
<ul>
<li>$t_{i+1}\ge t_i$</li>
<li>$\hat{t}_i\ge t_i$   if $\hat{t}_i$ is not missing</li>
<li>we assume that most timestamp estimates including the first and the last one are correct.</li>
</ul>
<p>How can we check if a timestamp series is non-increasing?</p>
<p>A: it is non-increasing everywhere. any adjacent pais is non-increasing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_non_increasing</span><span class="params">(times)</span>:</span></span><br><span class="line">    increase=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(times)<span class="number">-1</span>):</span><br><span class="line">        now=i</span><br><span class="line">        before=i+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> times[now]&lt;times[before]: <span class="comment"># there is a increase.</span></span><br><span class="line">            increase+=<span class="number">1</span> <span class="comment"># you can also stop and return something here</span></span><br><span class="line">    <span class="keyword">return</span> increase</span><br></pre></td></tr></table></figure>
<p>There are two extreme ways of correction:</p>
<ul>
<li>get the upper bound of corrected time</li>
</ul>
<p>for each correction, let it be as large as possible，向前看齊</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pd_merge[<span class="string">'CorrectedTime_U'</span>]=<span class="literal">None</span></span><br><span class="line">earliest_time=pd_merge.Time[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pd_merge.index:</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">not</span> pd.isna(pd_merge.Time[i]) <span class="keyword">and</span> pd_merge.Time[i] &lt;= earliest_time):</span><br><span class="line">        earliest_time = pd_merge.Time[i]</span><br><span class="line">    pd_merge.CorrectedTime_U[i] = earliest_time</span><br></pre></td></tr></table></figure>
<ul>
<li>get the lower bound of corrected time</li>
</ul>
<p>for each correction, let it be as small as possible, 向後看齊</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pd_merge[<span class="string">'CorrectedTime_L'</span>]=<span class="literal">None</span></span><br><span class="line">pd_merge[<span class="string">'fault'</span>]=<span class="literal">False</span></span><br><span class="line">earliest_time=pd_merge.Time[<span class="number">0</span>]<span class="comment"># the latest time observed so far</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> pd_merge.index:</span><br><span class="line">    <span class="keyword">if</span> pd.isna(pd_merge.Time[i]) <span class="keyword">or</span> pd_merge.Time[i] &gt; earliest_time:</span><br><span class="line">        pd_merge.fault[i]=<span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        earliest_time = pd_merge.Time[i]</span><br></pre></td></tr></table></figure>
<pre><code>D:\Anoconda\envs\Python\lib\site-packages\ipykernel_launcher.py:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">latest_time=pd_merge.Time[<span class="number">836</span>]<span class="comment"># the latest time observed so far</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">836</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pd_merge.fault[i]:</span><br><span class="line">        latest_time = pd_merge.Time[i]</span><br><span class="line">    pd_merge.CorrectedTime_L[i]=latest_time</span><br></pre></td></tr></table></figure>
<pre><code>D:\Anoconda\envs\Python\lib\site-packages\ipykernel_launcher.py:5: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  &quot;&quot;&quot;
</code></pre><p>check if there is still faults</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_non_increasing(pd_merge.CorrectedTime_L)</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check_non_increasing(pd_merge.CorrectedTime_U)</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_merge</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>Page</th>
      <th>audience</th>
      <th>comment</th>
      <th>presents</th>
      <th>share</th>
      <th>url</th>
      <th>Time</th>
      <th>CorrectedTime_U</th>
      <th>CorrectedTime_L</th>
      <th>fault</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>忘我</td>
      <td>http://changba.com/s/EXb4wphbN-GY464jDRV_KA</td>
      <td>64</td>
      <td>4</td>
      <td>8</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/9...</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-29 00:00:00</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real Friends</td>
      <td>http://changba.com/s/FBZMCWgrLY3sO7sUEFbgSg</td>
      <td>32</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>http://ksapuw.changba.com/userdata/userwork/81...</td>
      <td>NaN</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-29 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>When I Was Your Man</td>
      <td>http://changba.com/s/GZr_J5_BvVr4a-ftEiASmw</td>
      <td>29</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>http://upscuw.changba.com/1208653739.mp3</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-29 00:00:00</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Nasa</td>
      <td>http://changba.com/s/GZr_J5_BvVpGx_odmAqtBQ</td>
      <td>25</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>http://upscuw.changba.com/1208653711.mp3</td>
      <td>NaN</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-28 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Scars To Your</td>
      <td>http://changba.com/s/oTPTKN_3qXjKpIfapWDAAA</td>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://qiniuuwmp3.changba.com/1208653567.mp3</td>
      <td>NaN</td>
      <td>2019-12-29 00:00:00</td>
      <td>2019-12-28 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>832</th>
      <td>恒星流星</td>
      <td>http://changba.com/s/lxnKkg0S0RVrwOn6rjxZOg</td>
      <td>41</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/8...</td>
      <td>2013-08-05 00:00:00</td>
      <td>2013-08-02 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>833</th>
      <td>也许明天</td>
      <td>http://changba.com/s/z1FbhFD6pZ669cvjWIcqfA</td>
      <td>106</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/8...</td>
      <td>NaN</td>
      <td>2013-08-02 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>834</th>
      <td>剪爱</td>
      <td>http://changba.com/s/TIO4hN908E8Wndka1ntQpQ</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/7...</td>
      <td>2013-08-01 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>False</td>
    </tr>
    <tr>
      <th>835</th>
      <td>空白格</td>
      <td>http://changba.com/s/co-YKmeP2LT5THi79AR9gQ</td>
      <td>72</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/5...</td>
      <td>2013-08-15 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>True</td>
    </tr>
    <tr>
      <th>836</th>
      <td>Listen</td>
      <td>http://changba.com/s/BCpW1QzqtJp1bFGVkMadXw</td>
      <td>63</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>http://upuwmp3.changba.com/userdata/userwork/4...</td>
      <td>2013-08-01 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>2013-08-01 00:00:00</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>837 rows × 11 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd_merge.to_csv(<span class="string">'changba.csv'</span>,encoding=<span class="string">"utf-8-sig"</span>)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/13/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/13/SVM/" class="post-title-link" itemprop="url">Lagrange Duality and SVM</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-13 16:15:17" itemprop="dateCreated datePublished" datetime="2020-01-13T16:15:17-07:00">2020-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-15 20:38:07" itemprop="dateModified" datetime="2020-01-15T20:38:07-07:00">2020-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Duality-Theory"><a href="#Duality-Theory" class="headerlink" title="Duality Theory"></a><a href="http://www.onmyphd.com/?p=duality.theory" target="_blank" rel="noopener">Duality Theory</a></h1><h2 id="Lagrangian"><a href="#Lagrangian" class="headerlink" title="Lagrangian"></a>Lagrangian</h2><p>Given a standard form problem (not necessarily convex)</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x \in \mathcal{R}^p}\ f(x)& \\
{\rm subject \ to\ } g_i(x)&\le 0, i=1,\dots,n_g\\
h_i(x)&=0, i=1,\dots,n_h
\end{aligned} \tag{1}</script><p>The problem, which is defined as the <strong>primal problem</strong>, has equality and inequality constraints. Denote the primal feasible area described by constraint functions <script type="math/tex">g_i</script> and <script type="math/tex">h_i</script> as </p>
<script type="math/tex; mode=display">\mathcal{D}=(\cap_{i=1}^{n_h}dom\ h_i)\cap(\cap_{i=1}^{n_g}dom\ g_i)</script><p>We define the Lagrangian <script type="math/tex">L:\mathcal{R}^p \times \mathcal{R}_+^{n_g} \times \mathcal{R}^{n_h} \to \mathcal{R}</script></p>
<script type="math/tex; mode=display">\mathbf{L}(x,\lambda,\nu)= f(x) + \sum_{i=1}^{n_h}\nu_ih_i(x)+  \sum_{i=1}^{n_g}\lambda_ig_i(x)</script><p>This is a weighted sum of objective $f$ and constraint functions $g_i$ and $h_i$, where $\lambda_i \ge 0$ is Lagrange multiplier associated with $g_i(x) \le 0$ and $\nu_i$ is Lagrange multiplier associated with $h_i(x)=0$. If we define the primal function $p(x)$ as follow</p>
<script type="math/tex; mode=display">
p(x) = sup_{\lambda\ge 0,\nu}\mathbf{L}(x,\lambda,\nu) = \left\{
\begin{aligned} f(x) \qquad & {\rm if} \ x \in \mathcal{D}\\
+\infty \qquad & {\rm else}
\end{aligned}
\right.
\tag{2}</script><blockquote>
<p>Proof: if $g_i(x)\le 0, h_i(x)=0$ any positive $\lambda_i$ makes Lagrangian less while $\nu_i$ does not effect the Lagrangian. Hence, the Sup of Lagrangian must be $f(x)$. On the other hand, if $g_i(x) \ge 0$ or $h_i(x)=0$ for any $i$, we can set corresponding weight as infinity then the Lagrangian can be infinity.</p>
</blockquote>
<p>Then we have </p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x}\ &f(x) \\
{\rm subject\ to\ }&x\in \mathcal{D}
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm min}_{x}\ &p(x) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned}</script><p>Hence, the optimization (1) is equivalent to minimizing (2), which in other words is:</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x}\ &f(x) \\
{\rm subject\ to\ }&x\in \mathcal{D}
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm min}_{x}{\rm max}_{\lambda,\nu}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \tag{Primal problem}</script><p>The function $p(x)$ (or $f(x)$) is called <strong>primal function</strong> </p>
<h2 id="Duality"><a href="#Duality" class="headerlink" title="Duality"></a>Duality</h2><p>The <strong>primal problem</strong> maximize Lagrangian with respect to $\lambda,\nu$ first and then maximize with respect to $X$. We can define the <strong>dual problem</strong> by <strong>switching the order of minimize and maximize</strong></p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm max}_{\lambda,\nu}{\rm min}_{x}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned}</script><p>Now we define the <strong>dual function</strong>:</p>
<script type="math/tex; mode=display">q(\lambda,\nu)=\inf_x\mathbf{L}(x,\lambda,\nu)</script><p>The dual problem is equivalent to maximize dual function:</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm max}_{\lambda,\nu}\ &q(\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm max}_{\lambda,\nu}{\rm min}_{x}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \tag{Dual problem}</script><p>Then we have the the lower bound property:</p>
<script type="math/tex; mode=display">q(\lambda,\nu)=\inf_x\mathbf{L}(x,\lambda,\nu)\le \inf_{x\in \mathcal{D}}\mathbf{L}(x,\lambda,\nu) \le sup_{\lambda\ge 0,\nu}\mathbf{L}(x,\lambda,\nu) = f(x), x\in \mathcal{D}</script><p>It tells us that $q(λ,μ)$ is always smaller than $f(x)$ and, therefore, it serves as a lower bound for the <strong>primal function</strong>.</p>
<h2 id="Duality-Gap"><a href="#Duality-Gap" class="headerlink" title="Duality Gap"></a>Duality Gap</h2><p>Assume the optimal value of primal problem is $p^\star$. Given a dual feasible $\lambda,\nu$, we can establish a lower bound on the optimal value of primal problem: $p^\star \ge q(\lambda,\nu)$. Hence, we have</p>
<script type="math/tex; mode=display">0 \le f(x)-p^\star \le f(x) - q(\lambda,\nu)</script><p>Versus, assume optimal value of dual problem is $q^\star$, given a primal feasible $x$ we have</p>
<script type="math/tex; mode=display">0 \le q^\star - q(\lambda,\nu) \le f(x) - q(\lambda,\nu)</script><p>The difference $f(x) - q(\lambda,\nu)$ is called <strong>duality gap</strong> associated with the primal feasible point $x$ and dual feasible point $\lambda,\nu$ and is always non-negative. If we can find the primal dual feasible point $x^\star, \lambda^\star,\nu^\star$ such that the <strong>duality gap is zero</strong>, then $x^\star$ is primal optimal and $\lambda^\star,\nu^\star$ is dual optimal</p>
<blockquote>
<p>Brief proof. When duality gap is zero, $0 \le f(x^\star)-p^\star \le 0$. Hence $f(x^\star)=p^\star$</p>
</blockquote>
<p><strong>When the duality gap is zero, we say the problem has a strong duality, otherwise it has a weak duality. </strong> </p>
<blockquote>
<p>Strong duality does not hold in general but if the primal problem is convex i.e. of form with $f$ and $g_i$ convex, we usually (but not always) have strong duality. </p>
</blockquote>
<script type="math/tex; mode=display">\begin{aligned}
f(x^\star) &= q(\lambda^\star,\nu^\star) \\
&= {\rm inf}_{x}(f(x) + \sum_{i=1}^{n_h}\nu_ih_i(x)+  \sum_{i=1}^{n_g}\lambda_ig_i(x))\\
&\le f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star \underbrace{h_i(x^\star)}_{=0}+  \sum_{i=1}^{n_g}\underbrace{\lambda_i^\star}_{\ge 0} \underbrace{g_i(x^\star)}_{\le 0 }\\
&\le f(x^\star)
\end{aligned}</script><p>We can draw an important conclusion from above:</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n_g}\lambda_i^\star g_i(x^\star) =0</script><p>Since all terms are non-positive, we conclude that</p>
<script type="math/tex; mode=display">\lambda_i^\star g_i(x^\star) =0, \quad i=1,\dots,n_g \tag{2}</script><p>This term is known as complementary slackness; it holds any primal optimal $x^\star$ when strong duality holds. We can express this complementary slackness condition as </p>
<script type="math/tex; mode=display">\lambda_i^\star > 0 \to g_i(x^\star) =0</script><p>or equivalently,</p>
<script type="math/tex; mode=display">g_i(x^\star) < 0 \to \lambda_i^\star > 0</script><p>Roughly speaking, this means the $i$-th optimal Lagrange multiplier is zero unless the $i$-th constraint is active at the optimum.</p>
<p>Another conclusion we can draw is that since the third inequality is actually an equality, we conclude that the primal optimal $x^\star$ minimizes the Lagrange $L(x, \lambda^\star,\nu^\star)$ over $x$, which indicates that its gradient must vanish at $x^\star$</p>
<script type="math/tex; mode=display">\triangledown f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star\triangledown h_i(x^\star)+  \sum_{i=1}^{n_g}\lambda_i^\star\triangledown g_i(x^\star) =0 \tag{3}</script><h2 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h2><p>Summarize the conditions above we have: </p>
<script type="math/tex; mode=display">\begin{aligned}
g_i(x^\star) & \le 0, \quad i=1,\dots,n_g\\
h_i(x^\star) & = 0, \quad i=1,\dots,n_h\\
\lambda_i^\star &\ge 0, \quad i=1,\dots,n_g\\
\lambda_i^\star g_i(x^\star)& =0, \quad i=1,\dots,n_g\\
\triangledown f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star\triangledown h_i(x^\star)+  \sum_{i=1}^{n_g}\lambda_i^\star\triangledown g_i(x^\star) &=0
\end{aligned}</script><p>These are call <strong>Karush-Kuhn-Tucker (KKT) conditions</strong>. It consists of four parts:</p>
<ul>
<li>First two conditions comes from <strong>primal problem</strong></li>
<li>third one is <strong>non-negative constrain on Lagrange multiplier</strong>. </li>
<li>the 4-th condition follows equation (2) <strong>complementary slackness</strong>.</li>
<li>the last one follows equation (3) which is <strong>gradient</strong> vanishing constrain. </li>
</ul>
<p>The KKT conditions is necessary when the primal problem is non-convex and is also <strong>sufficient for points to be primal and dual optimal when when the primal problem is convex</strong>. </p>
<h1 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h1><p>Support Vector Machine (SVM) has achieved massive success and still keeps popular in many real-world situations especially in high-dimensional classification tasks. The main idea of SVM is to find the best hyperplane that separates two groups of data. So basically it is still a linear classifier. </p>
<p>Assuming $x_i\in \mathcal{R}^p$ and $y_i\in \lbrace -1,1 \rbrace$, a hyperplane was defined as </p>
<script type="math/tex; mode=display">\lbrace x:x^T\beta+\beta_0=0\rbrace</script><p>Then the classification function is </p>
<script type="math/tex; mode=display">g(x)=sign(x^T\beta+\beta_0)</script><p>The <strong>geometric margin</strong> from a point $(x_i,y_i)$ to the hyperplane is </p>
<script type="math/tex; mode=display">m_i = \frac{|x_i^T\beta+\beta_0|}{\|\beta\|}</script><p>$m_i$ measures the geometric distance from an observation to the hyperplane. </p>
<p>Hence, we aim to find the hyperplane maximizing the minimum geometric margin</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}m\\ \text{subject to } m_i\ge m,\ &i=1,...,n
\end{aligned}</script><p>where $m$ is the lower bound of geometric Margins. Alternatively, multiplied $||\beta||$  on both sides, the optimization problem is equivalent to the following form</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\begin{aligned}
&\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}m \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge m \|\beta\|
\end{aligned} 
\\
 m=\frac{M}{\|\beta\|} \downarrow &
\\
&\begin{aligned}
&\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{M}{\|\beta\|} \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge M,
\end{aligned}
\\
 M=1(i.e. \beta\leftarrow m \beta) \downarrow &
\\
&\begin{aligned}
&\min_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2 \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge 1,
\end{aligned}
\end{aligned}</script><p>The definition of margin origins from the constrain $|x_i^T\beta+\beta_0|\ge 1$. The <strong>functional margin</strong> of an observation $(x_i,y_i)$ with respect to the hyperplane is defined as </p>
<script type="math/tex; mode=display">M_i=|x_i^T\beta+\beta_0|=y_i(x_i^T\beta+\beta_0)</script><p>and we have our optimization problem as follow:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2\\ \text{subject to } M_i\ge 1,\ &i=1,...,n
\end{aligned}\tag{Primal problem}</script><h2 id="Slackness"><a href="#Slackness" class="headerlink" title="Slackness"></a>Slackness</h2><p>When two clusters are not linearly separable or even overlap, a soft constrain is introduced to allow for some points on the wrong side of the hyperplane. As shown below, the optimization problem is modified by introducing the slack variables $\xi$.    </p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2\\ \text{subject to } M_i&\ge 1-\xi_i,\ i=1,...,n\\
\xi_i&\ge 0,\ i=1,...,n;\\
\sum_{i=1}^{n}\xi_i&\le F
\end{aligned}</script><p>where $F$ is a constant controlling the total amount of slackness we allow. By introducing slack variables $\xi$, we allow small margins close to the hyperplane with $\xi\in[0,1]$ and even negative margins i.e. $yf(x)&lt;0$ with $\xi\ge 1$.      </p>
<h2 id="Dual-problem"><a href="#Dual-problem" class="headerlink" title="Dual problem"></a>Dual problem</h2><p>Obviously, this is a convex problem with convex objective function and linear constrain over $\beta,\beta_0$. To find the solution, we firstly get the Lagrangian as follow</p>
<script type="math/tex; mode=display">
L((\beta_0,\beta,\xi),(C,\alpha,\mu))= \frac{1}{2}\|\beta\|^2-\sum_{i=1}^{n}\alpha_i(M_i-(1-\xi_i))-\sum_{i=1}^{n}\mu_i\xi_i+C\sum_{i=1}^{n}\xi_i</script><p>where $\alpha_i,\mu_i,C$ are Lagrange multiplier for the constrains and $\xi_i$ are feasible variables and all of them must be positive. Since the constant is independent of any variables, we usually drop the constant term.</p>
<p>Then the primal problem is </p>
<script type="math/tex; mode=display">\min_{\beta_0,\beta,\xi}\max_{C,\alpha,\mu}L((\beta_0,\beta,\xi),(C,\alpha,\mu))</script><p>The corresponding <strong>Dual problem</strong> is </p>
<script type="math/tex; mode=display">\max_{C,\alpha,\mu}\min_{\beta_0,\beta,\xi}L((\beta_0,\beta,\xi),(C,\alpha,\mu))</script><p>By setting the gradient of Lagrangian w.r.t $\beta_0,\beta,\xi$ as 0 we have:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\triangledown_{\beta_0} L =0 &\to  0=\sum_{i=1}^{n}\alpha_iy_i,\\
\triangledown_{\beta} L =0 &\to \beta=\sum_{i=1}^{n}\alpha_iy_ix_i,\\
\triangledown_{\xi_i} L =0 &\to \alpha_i=C-\mu_{i},
\end{aligned}</script><p>we can cancel $\beta,\beta_0, \xi_i$ and also $\mu_i, C$ obtain the Lagrangian as below</p>
<script type="math/tex; mode=display">
\begin{aligned}
L((\beta_0,\beta,\xi),(C,\alpha,\mu)) 
&= \frac{1}{2}\|\beta\|^2
-\sum_{i=1}^{n}\alpha_i(M_i-(1-\xi_i))
-\sum_{i=1}^{n}\mu_i\xi_i
+C\sum_{i=1}^{n}\xi_i\\
&=\frac{1}{2}\|\sum_{i=1}^{n}\alpha_iy_ix_i\|^2 
-\sum_{i=1}^{n}\alpha_i(y_i(x_i^T(\sum_{j=1}^{n}\alpha_jy_jx_j)+\beta_0)-(1-\xi_i))
+\sum_{i=1}^{n}(C-\mu_i)\xi_i\\
&=\frac{1}{2}\|\sum_{i=1}^{n}\alpha_iy_ix_i\|^2
 -\sum_{i=1}^{n}\alpha_i(y_ix_i^T(\sum_{j=1}^{n}\alpha_jy_jx_j)
 -\sum_{i=1}^{n}\alpha_iy_i\beta_0
 +\sum_{i=1}^{n}\alpha_i
 -\sum_{i=1}^{n}\alpha_i\xi_i
 +\sum_{i=1}^{n}\alpha_i\xi_i\\
&=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j +\sum_{i=1}^{n}\alpha_i\\
&=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j
\end{aligned}</script><p>then <strong>Dual problem</strong> is as below</p>
<script type="math/tex; mode=display">
\max_{\alpha,\mu,C} \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
\text{subject to}\sum_{i=1}^{n}\alpha_iy_i=0</script><p>where the only variable is $\alpha$. Assume we find the unique solution $\alpha_i^*$ for the dual problem. The slope for hyperplane is </p>
<script type="math/tex; mode=display">\beta^\star=\sum_{i=1}^{n}\alpha_i^\star y_ix_i</script><p>Notice that the slope is only determined by a linear combinations of $y_ix_i$ and those observations with non-zero weights $\alpha_i\ne 0$ is called support vectors.</p>
<p>To determine a the hyperplane we also <a href="https://zhuanlan.zhihu.com/p/62420593" target="_blank" rel="noopener">require the intercept</a> $\beta_0^*$</p>
<h2 id="Sequential-minimal-optimization-SMO"><a href="#Sequential-minimal-optimization-SMO" class="headerlink" title="Sequential minimal optimization (SMO)"></a>Sequential minimal optimization (SMO)</h2><h2 id="KKT-conditions"><a href="#KKT-conditions" class="headerlink" title="KKT conditions:"></a>KKT conditions:</h2><p>Now we check the KKT conditions:</p>
<ul>
<li>constrains comes from <strong>primal problem</strong><script type="math/tex; mode=display">\begin{aligned}
M_i-(1-\xi_i)&\ge 0\\
\xi_i&\ge 0\\
\sum_{i=1}^{n}\xi_i-F&\le 0
\end{aligned}</script></li>
<li><strong>non-negative constrain on Lagrange multiplier</strong>.<script type="math/tex; mode=display">\begin{aligned}
\alpha_i&\ge 0\\
\mu_i&\ge 0\\
C &\ge 0
\end{aligned}</script></li>
<li><strong>complementary slackness</strong>.<script type="math/tex; mode=display">\begin{aligned}
\alpha_i(M_i-(1-\xi_i))&=0\\
\mu_i\xi_i&=0\\
C(\sum_{i=1}^{n}\xi_i-F)&=0\\
\end{aligned}</script></li>
<li><strong>gradient</strong> vanishing constrain. (must be true by setting gradient as zero)<script type="math/tex; mode=display">
\begin{aligned}
\beta_0: 0&=\sum_{i=1}^{n}\alpha_iy_i,\\
\beta_i: \beta&=\sum_{i=1}^{n}\alpha_iy_ix_i,\\
\xi_i: \alpha_i&=C-\mu_{i},
\end{aligned}</script></li>
</ul>
<p>Ideally we can use the 6 equations from <strong>complementary slackness</strong> and <strong>gradient</strong> vanishing constrain to <strong>find the one and only optimal solution and verify the other inequalities</strong>, however, it is hard to solve these equations. Although it seems <strong>impossible to get a closed form of optimal solution</strong>, which is why we search it by SMO with iterations, (so far I don’t know how to solve these equations), we can tell that with <strong>six equations and six variables, there must be a solution</strong>. Assuming the solution also follows the inequalities, we can draw the conclusion that the optimal solution we get for dual problem is also for primal problem.</p>
<p>Actually, it is <strong>not necessary to verify the KKT conditions</strong> since <strong>even if it is not satisfied, the optimal solution of dual problem is still a good estimation of primal problem.</strong></p>
<h1 id="Kernal-trick"><a href="#Kernal-trick" class="headerlink" title="Kernal trick"></a>Kernal trick</h1><p>So far the optimization is still searching for a linear classifier with the hyperplane defined as ${x:\ f(x)=x^T\beta+\beta_0=0}$ a linear from with respect to x. The “Kernel trick” makes it possible for linear boundaries to lie in a higher dimensional space and to be nonlinear projected on original space. </p>
<p>We define the basis functions as $h<em>m(x)=,m=1,…,M$ and represent all observations in the new space $h(x_i)=(h_1(x_i),h_2(x_i),…h_M(x_i))$. The hyperplane in this new space is ${x:\ f’(x)=f(h(x))=\sum</em>{i=1}^{n}\alpha_i^<em>y_ih(x_i)^Th(x) + \beta_0^</em>=0}$ and the classification function is still $g(x)=sign(f’(x))$. As observed from the form of hyperplane, the equation is a linear combination of the inner product of $h(x_i)$ and $h(x)$, which is called kernel function.</p>
<script type="math/tex; mode=display">
\mathcal{K}(x,y)=h(x)^Th(y)</script><p>There are many types of kernel functions such as polynomial, Gaussian, radial basis and splines. </p>
<h1 id="Why-Dual-Problem"><a href="#Why-Dual-Problem" class="headerlink" title="Why Dual Problem?"></a>Why Dual Problem?</h1><ul>
<li>Dual problem has no inequality constrain</li>
<li>The complexity of primal problem is proportional to dimension while that of dual problem only matters with sample size $n$</li>
<li>Kernel trick makes dimension even higher.</li>
</ul>
<h1 id="Multi-classification"><a href="#Multi-classification" class="headerlink" title="Multi-classification"></a>Multi-classification</h1><p>SVM is also capable of multi-classification and there are three ways based on binary classifiers: <code>one-against-all&#39;&#39;,</code>one-against-one’’, and  Directed Acyclic Graph Support Vector Machines (DAGSVM) . The first one builds $K$ SVM classifiers for $K$ classes respectively with data from $k$-th class as positive and from other classes as negative. Then we take the margins as score and we classify a new observation with the largest margin. The second method constructs $K(K - 1)/2$ classifiers where each one is trained on data from two classes pair-wisely. For a new observation, we use the voting strategy: with votes for all classes initialized as zero, we add one to $k$-th class’s vote if the result from one of these classifiers is $k$-th class, then the conclusion is the class with greatest votes. Though shares the same training steps as ``one-against-one’’ method with $K(K - 1)/2$ SVM classifiers, the third one DAGSVM is more complicated and delicate with a tree based decision function.</p>
<p>One thing we need to pay attention to is its sensitivity to prior. SVM is sensitive to both noise and prior. It tends to label the new observation into the class with more samples. This is somehow undesirable when we care more about the minority instead of majority. This property is illustrated in simulation where we checked the sensitivity of all methods we used.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/07/square-root/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/07/square-root/" class="post-title-link" itemprop="url">Methods of computing square roots</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-07 21:46:07 / Modified: 22:13:50" itemprop="dateCreated datePublished" datetime="2020-01-07T21:46:07-07:00">2020-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Babylonian-method"><a href="#Babylonian-method" class="headerlink" title="Babylonian method"></a><a href="https://en.wikipedia.org/wiki/Methods_of_computing_square_roots" target="_blank" rel="noopener">Babylonian method</a></h1><p>To get the square root of $S$, it proceeds as follows:</p>
<ol>
<li>Begin with an arbitrary positive starting value $x_0$ (the closer to the actual square root of $S$, the better). $n=0$<script type="math/tex; mode=display">x_0\approx \sqrt{S}</script></li>
<li>Let $x_{n+1}$ be the average of $x_n$ and $\frac{S}{x_n}$ using the arithmetic mean to approximate the geometric mean).<script type="math/tex; mode=display">x_{n+1}=\frac{1}{2}(x_n+\frac{S}{x_n})</script></li>
<li>Repeat step 2 until the desired accuracy is achieved.</li>
</ol>
<p>Why it makes sense.</p>
<p>assuming $x$ is the square root of $S$, $f(x)=x^2$, order one Taylor expansion at $x_n$ is: </p>
<script type="math/tex; mode=display">\begin{aligned}
f(x) &\approx f(x_n) + f^1(x_n)(x - x_n) \\
&= x_n^2 + 2x_n(x-x_n)
\end{aligned}</script><p>Let $f(x) = S$ we have:</p>
<script type="math/tex; mode=display">xx_n = \frac{1}{2}(x_n^2+S)</script><h1 id="Binary-search"><a href="#Binary-search" class="headerlink" title="Binary search"></a>Binary search</h1><p>The square root of a positive number must lies between this number and $1$. Use binary search to find the upper and lower limits by iterations.</p>
<ul>
<li>easier to control the granularity by checking the difference between upper and lower limits</li>
<li>not efficient</li>
</ul>
<h1 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/07/python-increment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/07/python-increment/" class="post-title-link" itemprop="url">Behaviour of increment and decrement operators in Python</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-07 18:47:56 / Modified: 22:14:50" itemprop="dateCreated datePublished" datetime="2020-01-07T18:47:56-07:00">2020-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>What is the behavior of the pre-increment/decrement operators (++/—) in Python? </p>
<p>When you want to increment or decrement, you typically want to do that on an integer. Like so:</p>
<pre><code>b++
</code></pre><p>But in Python, integers are immutable. That is you can’t change them. This is because the integer objects can be used under several names. Try this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; b = 5</span><br><span class="line">&gt;&gt;&gt; a = 5</span><br><span class="line">&gt;&gt;&gt; id(a)</span><br><span class="line">162334512</span><br><span class="line">&gt;&gt;&gt; id(b)</span><br><span class="line">162334512</span><br><span class="line">&gt;&gt;&gt; a is b</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p>a and b above are actually the same object. If you incremented a, you would also increment b. That’s not what you want. So you have to reassign. Like this:</p>
<pre><code>b = b + 1
</code></pre><p>Or simpler:</p>
<pre><code>b += 1
</code></pre><p>Which will reassign b to b+1. That is not an increment operator, because it does not increment b, it reassigns it.</p>
<p>In short: Python behaves differently here, because it is not C, and is not a low level wrapper around machine code, but a high-level dynamic language, where increments don’t make sense, and also are not as necessary as in C, where you use them every time you have a loop, for example.</p>
<p><a href="https://stackoverflow.com/questions/1485841/behaviour-of-increment-and-decrement-operators-in-python" target="_blank" rel="noopener">Reference</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/02/GMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/02/GMM/" class="post-title-link" itemprop="url">Gaussian Mixture Model (GMM) and Expectation–Maximization (EM)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-02 21:44:47" itemprop="dateCreated datePublished" datetime="2020-01-02T21:44:47-07:00">2020-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-04 01:12:31" itemprop="dateModified" datetime="2020-01-04T01:12:31-07:00">2020-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Gaussian Mixture Model is one of clustering model. Clustering is an unsupervised learning problem where we intend to find clusters of points in our dataset that share some common characteristics.  </p>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><p>$\mathcal{X}$ input space ($p$ dimensions)<br>$\mathbf{x}$ input variable $\mathbf{x} \in \mathcal{X}$<br>$n$ train sample size<br>$\mathbf{x}_i$ training input, $i= 1, \dots, n$<br>$d(\mathbf{x}_i,\mathbf{x}_j)$ distance between $\mathbf{x}_i,\mathbf{x}_j$<br>$K$ number of clusters<br>$\mathcal{C}(\mathbf{x}_i) = k$ encoder that assigns $\mathbf{x}_i$ to cluster $k$, $k\in \lbrace 1,\dots K\rbrace $<br>$n_k$ sample size for cluster $k$</p>
<p>The goal of clustering is to partition observations into groups (clusters) so that similar observations should be assigned to same cluster. Assume we expect to have in total $K$ clusters and each one is labeled by an integer $k\in \lbrace 1,\dots K\rbrace$</p>
<ul>
<li>Combinatorial algorithm </li>
</ul>
<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><p>Combinatorial algorithm assgins each observation to <strong>one and only one group or cluster </strong>without regard to a underlying probability model. This assignment can be characterized by a encoder $\mathcal{C}(\mathbf{x}_i) = k$. Combinatorial algorithm aims to find the particular encoder such that </p>
<script type="math/tex; mode=display">\min \frac{1}{2}\sum_{k=1}^K \sum_{\mathcal{C}(\mathbf{x}_i) = k} \sum_{\mathcal{C}(\mathbf{x}_j) = k} d(\mathbf{x}_i,\mathbf{x}_j)</script><p>This criterion characterize the extent to which observations assigned to the same cluster tend to be close to another one. It can also be interpreted as “within cluster” distance if we have the decomposition of total pairwise distance as follow</p>
<script type="math/tex; mode=display">\begin{aligned}
T=& \frac{1}{2} \sum_{k=1}^K \sum_{i=1}^n \sum_{j=1}^n d(\mathbf{x}_i,\mathbf{x}_j) \\
 =& \frac{1}{2} \sum_{k=1}^K \sum_{\mathcal{C}(\mathbf{x}_i) = k} (\sum_{\mathcal{C}(\mathbf{x}_j) = k} d(\mathbf{x}_i,\mathbf{x}_j) + \sum_{\mathcal{C}(\mathbf{x}_j) \ne k} d(\mathbf{x}_i,\mathbf{x}_j))\\
 =& W(\mathcal{C}) + B(\mathcal{C})
\end{aligned}</script><p>where $W(\mathcal{C})$ is the loss function introduced above called “within cluster” distance and $B(\mathcal{C})$ represents “between cluster” distance. Since $T$ is a constant value given data, minimizing $W(\mathcal{C})$ is equivalent to maximizing $B(\mathcal{C})$</p>
<p>K-means algorithm is one of the most popular iterative descent clustering method and it defines distance function as squared Euclidean distance.</p>
<script type="math/tex; mode=display">d(\mathbf{x}_i,\mathbf{x}_j)=\sum_{q=1}^p(x_{iq}-x_{jq})^2 = ||\mathbf{x}_i-\mathbf{x}_j||^2</script><p>Then for each cluster, “within cluster” distance can be simplified</p>
<script type="math/tex; mode=display">\begin{aligned}
&\frac{1}{2}\sum_{\mathcal{C}(\mathbf{x}_i) = k}\sum_{\mathcal{C}(\mathbf{x}_j) = k} ||\mathbf{x}_i-\mathbf{x}_j||^2\\ 
=&\frac{1}{2}\sum_{\mathcal{C}(\mathbf{x}_i) = k}\sum_{\mathcal{C}(\mathbf{x}_j) = k} ||\mathbf{x}_i- \bar{\mathbf{x}}_k - (\mathbf{x}_j -  \bar{\mathbf{x}}_k) ||^2\\
=&\frac{1}{2}\sum_{\mathcal{C}(\mathbf{x}_i) = k}\sum_{\mathcal{C}(\mathbf{x}_j) = k} ||\mathbf{x}_i- \bar{\mathbf{x}}_k||^2  + ||(\mathbf{x}_j -  \bar{\mathbf{x}}_k) ||^2 -2(\mathbf{x}_i- \bar{\mathbf{x}}_k)^T(\mathbf{x}_j -  \bar{\mathbf{x}}_k)\\
=&\frac{1}{2}\sum_{\mathcal{C}(\mathbf{x}_i) = k} n_k ||\mathbf{x}_i- \bar{\mathbf{x}}_k||^2 +\frac{1}{2}n_k \sum_{\mathcal{C}(\mathbf{x}_j) = k}  ||\mathbf{x}_i- \bar{\mathbf{x}}_k||^2\\& + \sum_{\mathcal{C}(\mathbf{x}_i) = k}(\mathbf{x}_i- \bar{\mathbf{x}}_k)^T \sum_{\mathcal{C}(\mathbf{x}_j) = k}(\mathbf{x}_j - \bar{\mathbf{x}}_k) \\
=& n_k \sum_{\mathcal{C}(\mathbf{x}_j) = k}  ||\mathbf{x}_i- \bar{\mathbf{x}}_k||^2 
\end{aligned}</script><p>where $n_k$ is the sample size of cluster $k$ and $\bar{\mathbf{x}}_k$ is the mean vector of cluster $k$. The loss function can be written as</p>
<script type="math/tex; mode=display">W(\mathcal{C})=\sum_{k=1}^K n_k \sum_{\mathcal{C}(\mathbf{x}_j) = k}  ||\mathbf{x}_i- \bar{\mathbf{x}}_k||^2</script><p>However, for either original loss function or this one using Euclidean distance, it is time consuming with respect to sample size and number of clusters if we enumerate all possible clustering combination.</p>
<script type="math/tex; mode=display">S(n,K)=\frac{1}{K!}\sum_{k=1}^{K}(-1)^{K-k}\dbinom{K}{k}k^n</script><p>Hence, most clustering models use an iterative greedy descent strategy to find a suboptimal partition. We first initialize a partition and at each iterative step the partition are modified in a way such that loss function (or any criterion) is improved from previous value. </p>
<h2 id="training-algorithm"><a href="#training-algorithm" class="headerlink" title="training algorithm"></a>training algorithm</h2><ol>
<li>Initialize a cluster assignment $\mathcal{C^0}$</li>
<li>For a given assignment $\mathcal{C^t}$, calculate cluster means $\mu_k^t = \bar{\mathbf{x}}_k$ (which minimized the cluster variance)</li>
<li>Given cluster means $\mu_k^t$, update the assignment $\mathcal{C^{t+1}}$ that assign each observation to the closest cluster mean<script type="math/tex; mode=display">\mathcal{C}^{t+1}(\mathbf{x})=\arg \min_{k}||d(\mathbf{x},\mu_k^t)||</script></li>
<li>if $\mathcal{C^t}=\mathcal{C}^{t+1}$: return $\mathcal{C}^{t+1}$<br>else: t=t+1, go back to step 2.</li>
</ol>
<h1 id="Gaussian-Mixture-Model-GMM"><a href="#Gaussian-Mixture-Model-GMM" class="headerlink" title="Gaussian Mixture Model (GMM)"></a>Gaussian Mixture Model (GMM)</h1><p>Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. GMMs have been used for <strong>feature extraction from speech data</strong>, and have also been used extensively in <strong>object tracking</strong> of multiple objects, where the number of mixture components and their means predict object locations at each frame in a video sequence.<a href="https://brilliant.org/wiki/gaussian-mixture-model/" target="_blank" rel="noopener">GMM</a></p>
<h2 id="Gaussian-distribution"><a href="#Gaussian-distribution" class="headerlink" title="Gaussian distribution"></a>Gaussian distribution</h2><p>Recall that a (Multi-dimensional) Gaussian distribution is determined by two parameters: mean vector $\mu$ and covariance matrix $\Sigma$. The density function $\mathcal{N}(\mathbf{x}|\mu,\Sigma)$ is</p>
<script type="math/tex; mode=display">\mathcal{N}(\mathbf{x}|\mu,\Sigma)
=\frac{1}{\sqrt{2\pi|\Sigma|}}
\exp(-\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu))</script><p>We can fit a single Gaussian distribution to a dataset $\mathbf{D}=\lbrace\mathbf{x}_i \rbrace$ by maximizing the likelihood (if no prior) and get the MLE estimator of $\mu$ and $\Sigma$.</p>
<script type="math/tex; mode=display">\begin{aligned}
L(\mu,\Sigma|\mathbf{D})=&\prod_{i=1}^n \mathcal{N}(\mathbf{x}|\mu,\Sigma)\\
=& (2\pi|\Sigma|)^{-\frac{n}{2}} \exp (\sum_{i=1}^n -\frac{1}{2}(\mathbf{x}_i-\mu)^T\Sigma^{-1}(\mathbf{x}_i-\mu) )
\end{aligned}</script><p>We know that the optimal parameter estimate is </p>
<script type="math/tex; mode=display">\hat{\mu}= \bar{\mathbf{x}}_i</script><script type="math/tex; mode=display">\hat{\Sigma}= \frac{1}{n}\sum_{i=1}^n (\mathbf{x}_i - \hat{\mu})(\mathbf{x}_i - \hat{\mu})^T</script><h2 id="Gaussian-Mixture"><a href="#Gaussian-Mixture" class="headerlink" title="Gaussian Mixture"></a>Gaussian Mixture</h2><p>In clustering scenario, we have data clustered with patterns and it is absolutely bad idea to fit our data in one Gaussian Distribution. Instead, we assume we have several Gaussian Distributions with different parameters and our data are sampled from a distribution mixed by them. </p>
<p>The mixture is imposed on density function</p>
<script type="math/tex; mode=display">p(x|\mu_{\cdot},\Sigma_{\cdot},\alpha_{\cdot})=\sum_{k=1}^K\alpha_k\mathcal{N}(\mathbf{x}|\mu_k,\Sigma_k), \ \sum_{k=1}^K\alpha_k=1</script><p>We can still fit a Gaussian Mixture distribution to a dataset $\mathbf{D}=\lbrace\mathbf{x}_i \rbrace$ by maximizing the likelihood</p>
<script type="math/tex; mode=display">\begin{aligned}
L(\mu_k,\Sigma_k,\alpha_k|\mathbf{D})=&\prod_{i=1}^n p(\mathbf{x}|\mu_{\cdot},\Sigma_{\cdot},\alpha_{\cdot})\\
=& 
\end{aligned}</script><h1 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank" rel="noopener">The Elements of Statistical Learning</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/25/Hyperparameter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/25/Hyperparameter/" class="post-title-link" itemprop="url">Hyperparameter tuning</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-25 21:49:08" itemprop="dateCreated datePublished" datetime="2019-12-25T21:49:08-07:00">2019-12-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-27 23:48:33" itemprop="dateModified" datetime="2019-12-27T23:48:33-07:00">2019-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/23/Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/23/Ensemble/" class="post-title-link" itemprop="url">Ensemble learning</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-23 22:22:08" itemprop="dateCreated datePublished" datetime="2019-12-23T22:22:08-07:00">2019-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-13 16:18:26" itemprop="dateModified" datetime="2020-01-13T16:18:26-07:00">2020-01-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In statistics and machine learning, <a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener">Ensemble method</a> use <strong>multiple learning algorithms</strong> to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.<br>This post is mainly about the theory behind these big names. </p>
<h1 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h1><ul>
<li>data<br>$\mathcal{X}$ input space ($p$ dimensions) $\mathcal{Y}$ output space<br>$\mathbf{x}$ input variable $\mathbf{x} \in \mathcal{X}$<br>$n$ train sample size<br>$\mathbf{x}_i, y_i$ training input and output, $i= 1, \dots, n$<br>$w_i$ weight for sample $\mathbf{x}_i, y_i$<br>$\mathbf{D}=(\mathbf{X},\mathbf{y})$ is the design matrix $(\mathbf{x}_1,\dots, \mathbf{x}_n)^T$ and output vector $(y_1,\dots, y_n)^T$<br>$\mathcal{RC}(D)$ random choice from dataset $D$ <strong>with no replacement</strong><br>$\mathcal{RS}(D)$ random sampling from dataset $D$ <strong>with replacement</strong></li>
<li>learners<br>$f: \mathcal{X} \to \mathcal{Y}$ arbitrary function<br>$\mathcal{H}$ base learner space<br>$g: \mathcal{X} \to \mathcal{Y}$ and $g \in \mathcal{H}$ a base learner.<br>$g(\mathbf{x}|\mathbf{D})$ a base learner trained with \mathbf{D}<br>$K$ number of base learners<br>$\alpha_k$ weight for $g_k$, $k=1,\dots, K$</li>
<li>ensemble<br>$f_{*}^t: \mathcal{X} \to \mathcal{Y}$ The * ensemble with first $t$ base learners</li>
<li>loss function<br>$L(y,\hat{y})$ loss function comparing $y,\hat{y}$<br>$\mathbf{L}(*|\mathbf{D})$ loss target for * given $\mathbf{D}$<br>$\Omega(f)$ regularization function on $f$</li>
<li>CART<br>$R_j^t$ the j-th region partitioned by CART $g_t$<br>$J_t$ the total number of partitions  by CART $g_t$</li>
</ul>
<img src="/2019/12/23/Ensemble/Notation.jpg" class="" width="500" title="notation for figs">
<hr>
<p>Given training data $\mathbf{D}$, regression task aims to find a regression function </p>
<script type="math/tex; mode=display">y=g(\mathbf{x}|\mathbf{D})</script><p>when $\mathcal{Y} = \mathbf{R}$. Similarly, classification results in a multivariate function that estimate the probabilities (or equivalent) of a given input belonging to every class when $\mathcal{Y}$ is a finite set of categories and returns the category with largest prob.</p>
<script type="math/tex; mode=display">y=\arg max_y P(\mathbf{x}|y,\mathbf{D}) = which\ max \mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>, $\mathcal{P}(\mathbf{x}|\mathbf{D})=(P(\mathbf{x}|1,\mathbf{D}),\dots, P(\mathbf{x}||\mathcal{Y}|,\mathbf{D}))^T$ denoted as $g(\mathbf{x}|\mathbf{D})$ which returns the vector of probs for all categories $1,\dots,|\mathcal{Y}|$. Hense, classification task can also be taken as multi-variate linear regression where the response variable is $\mathcal{P}(\mathbf{x}|\mathbf{D})$.</p>
<p>Either for regression or classification, we build multiple models and these models (weak tho) are called base learner. Ensemble learning integrated these base learner by taking a linear additive combination of $g_k(\mathbf{x}|D_k)$.</p>
<script type="math/tex; mode=display">f_{ensemble}=\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}|D_k)</script><img src="/2019/12/23/Ensemble/Ensemble.jpg" class="" width="400" title="ensemble">
<h1 id="Hard-amp-soft-voting"><a href="#Hard-amp-soft-voting" class="headerlink" title="Hard &amp; soft voting"></a>Hard &amp; soft voting</h1><p>Voting is an idea specific for classification. Every base learner is trained on the <strong>whole dataset</strong> $D_k=\mathbf{D}$ and averaged with <strong>equal weight</strong> $\alpha_k=1/k$.</p>
<script type="math/tex; mode=display">f_{voting}=\sum_{k=1}^{K}\frac{1}{K}g_k(\mathbf{x}|\mathbf{D})=mean(g_k(\mathbf{x}|\mathbf{D}))</script><p>The soft voting is directly taking the average of prob vectors</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathcal{P}(\mathbf{x}|\mathbf{D})</script><p>while the hard voting instead takes the average of one-hot vector transformed from probs vectors.</p>
<script type="math/tex; mode=display">g_k(\mathbf{x})=\mathbf{1}(\mathcal{P}(\mathbf{x}|\mathbf{D})==\max(\mathcal{P}(\mathbf{x}|\mathbf{D}))</script><p>For example, in one binary classification task we ensemble three classifiers, the prob that an observation belongs to class 1 equal (0.45, 0.45, 0.9). With soft voting the average prob is (0.45 + 0.45 + 0.9)/3 = 0.6 while hard voting will return the average prob equal (0 + 0 + 1)/3 = 0.33.</p>
<h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bootstrap aggregating, often abbreviated as bagging, have each base learner in the ensemble vote with <strong>equal weight</strong> $\alpha_k=1/k$. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn <strong>subset of the training set</strong> $D_k=\mathcal{RS}(\mathbf{D})$. (In statistics, bootstrapping is any test or metric that relies on <strong>random sampling with replacement</strong>.)</p>
<script type="math/tex; mode=display">f_{bagging}=mean(g_k(\mathbf{x}|\mathcal{RS}(\mathbf{D})))</script><h2 id="OOB-score"><a href="#OOB-score" class="headerlink" title="OOB score??"></a>OOB score??</h2><img src="/2019/12/23/Ensemble/bagging.jpg" class="" width="400" title="bagging">
<h2 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h2><p>Random forest consists of more randomness. With <strong>base learner space $\mathcal{H}$ being CART</strong>(classification and regression tree), not only is <strong>training set randomly sampled</strong> piece by piece, but also a <strong>features subset is selected randomly</strong> </p>
<script type="math/tex; mode=display">D_k=\mathcal{RS}((\mathbf{X}_{:,\mathcal{RC}(1\dots,p)},\mathbf{y}))</script><img src="/2019/12/23/Ensemble/rf.jpg" class="" width="500" title="random forrest">
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Boosting involves <strong>incrementally</strong> building an ensemble by training each new model instance to <strong><del>emphasize</del> correct the training instances mis-classified by previous models</strong>. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data.<br>Target is to reduce the Loss function</p>
<script type="math/tex; mode=display">\mathbf{L}(f|\mathbf{D}) = \sum_i^{n}L(y_i,f(\mathbf{x}_i))</script><script type="math/tex; mode=display">f_{boost}=\arg \min \mathbf{L}(\mathbf{\alpha},\mathbf{g}|\mathbf{D}) = \arg \min_{\mathbf{\alpha},\mathbf{g}}\sum_i^{n}L(y_i,\sum_{k=1}^{K}\alpha_kg_k(\mathbf{x}_i))</script><p>It is computationally impossible to optimize the target function with $K$ learners <script type="math/tex">g_k</script> and weights $\alpha_k$ at the same time. Hence, we take the idea of <strong>greedy approach</strong> and optimize the target function <strong>sequentially</strong>. We start from training a weak learner $g_1$ and add more learners one by one to further reduce the loss function. </p>
<p>Given first $t-1$ learners ensemble optimized <script type="math/tex">f_{boost}^{t-1}=\sum_{k=1}^{t-1}\alpha_k g_k(X)</script>, the k-th learner can be obtained by optimizing:</p>
<script type="math/tex; mode=display">\min_{\alpha_t}\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i))</script><script type="math/tex; mode=display">\alpha_t, g_t = \arg \min_{\alpha, g} \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha g(\mathbf{x}_i))</script><h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><p>The idea of gradient boosting is to apply a <strong>steepest descent</strong> step to this minimization problem. For arbitrary <strong>differentiable loss function</strong> $L$, </p>
<script type="math/tex; mode=display">\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}} = f(y,f_{boost}^{t-1}(\mathbf{x})) \doteq r^t</script><p>This gradient value is called pseudo-residuals <script type="math/tex">r^t</script> and it can be calculated for every sample $r_i^t$. In regression task, when loss function is MSE, pseudo-residual is actually real residual.</p>
<script type="math/tex; mode=display">L(y,g)=(y-g)^2 \\ \frac{\partial L(y,g)}{\partial g} = -2(y-g)</script><p>In classification task,</p>
<script type="math/tex; mode=display">?</script><p>Let the t-th learner <script type="math/tex">g_t = - r_i^t</script>, adding $g_t$ to must reduce the loss function</p>
<script type="math/tex; mode=display">\sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) \ge \sum_i^{n}(y_i,f_{boost}^{t-1}(\mathbf{x}_i)- \alpha_t r_i^t)</script><p>Ideally we wish to find a learner such that <script type="math/tex">g_t = - r_i^t</script> is true every where, but this is even not possible for all observed samples or worthwhile since this is just a intermediate step during iterations. However, we can still use pseudo-residuals to guide the training of <script type="math/tex">g_t</script>. More concretely, we take pseudo-residuals of <script type="math/tex">f_{boost}^{t-1}</script> as response variable.</p>
<script type="math/tex; mode=display">D_t=\{(\mathbf{x}_i, r_i^t)\}</script><p>After training $g_k$ we still need to determine a good weight $\alpha_k$ for it. It might be impossible to find a closed form of optimal solution but there are still ways to find one good enough. Line search is one of the basic ways.</p>
<img src="/2019/12/23/Ensemble/gb.jpg" class="" width="500"> 
<p>In general, Gradient Boosting includes two stages in each iteration: </p>
<ol>
<li>k-th learner <script type="math/tex">g_t</script> is trained by pseudo-residuals (training set <script type="math/tex">D_t=\{(\mathbf{x}_i, r_i^t)\}</script>) </li>
<li>Line search <script type="math/tex">\alpha_t = \arg \min_{\alpha} \sum_i^{n}(y_i,f_{GB}^{t-1}+\alpha g_t)</script></li>
</ol>
<h3 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting Decision Tree (GBDT)"></a>Gradient Boosting Decision Tree (GBDT)</h3><p>Take decision tree (CART) being weak learner space as example. For this special case, Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner. (by introducing <strong>leaf-wise weight</strong> on base tree learner) Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region. Using the indicator notation, the output of <script type="math/tex">g_t</script> for input <script type="math/tex">\mathbf{x}</script> can be written as the sum: </p>
<script type="math/tex; mode=display">g_t(\mathbf{x})=\sum_{j=1}^{J_t}b_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p>where $b_j^t$ is the value predicted in the region $R_j^t$. (The partitions for different tree $g_t$ should be distinct otherwise there is no potential to improve the boosting function.) Then Line search </p>
<script type="math/tex; mode=display">\alpha_t = \arg \min_{\alpha} \sum_i^n L(y_i,f_{GB}^{t-1}+\alpha g_t)</script><p>Friedman proposes to modify this algorithm so that it chooses a separate optimal value $ \alpha_j^t$ for each of the tree($g_t$)`s regions $R_j^t$, rather than a single $\alpha_t$ for the whole tree. He calls the modified algorithm “TreeBoost”. This is equivalent to training the leaves weight $b_j^t$ according to the overall loss function rather than taking the average or voting result as weight (what CARTs do). However, the <strong>partition of $R_j^t$ is still determined by rules used in CART, which is irrelevant to loss function</strong>.</p>
<script type="math/tex; mode=display">\alpha = \arg \min_{\alpha} \sum_j^{J_t} \sum_{\mathbf{x}_i \in \mathbf{R}_j^t} L(y_i,f_{GB}^{t-1}+\alpha_j^t (\mathbf {1} _{R_j^t}(\mathbf{x}_i)))</script><h3 id="Stochastic-Gradient-Boosting-SGB"><a href="#Stochastic-Gradient-Boosting-SGB" class="headerlink" title="Stochastic Gradient Boosting (SGB)"></a>Stochastic Gradient Boosting (SGB)</h3><p>Friedman proposed a minor modification to the algorithm, motivated by bootstrap aggregation (“bagging”) method at each iteration of the algorithm, a base learner should be fit on a <strong>subsample of the training set</strong> drawn at random without replacement. It not only introduces randomness into the algorithm and help <strong>prevent overfitting</strong>, but also makes <strong>training faster</strong>, because regression trees have to be fit to smaller datasets at each iteration. He obtained that 0.5 ≤ f ≤ 0.8  leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.</p>
<h2 id="XGboost"><a href="#XGboost" class="headerlink" title="XGboost"></a>XGboost</h2><p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener">XGBoost</a> stands for Extreme Gradient Boosting; it is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best <strong>tree </strong>model. It employs a number of nifty tricks that make it exceptionally successful, particularly with structured data. </p>
<ul>
<li>computing second-order gradients, i.e. second partial derivatives of the loss function (similar to Newton’s method), which provides more information about the direction of gradients and how to get to the minimum of our loss function.</li>
<li>advanced regularization (L1 &amp; L2), which improves model generalization.</li>
<li>training is very fast and can be parallelized / distributed across clusters.</li>
</ul>
<h3 id="Regularization-on-Model-Complexity"><a href="#Regularization-on-Model-Complexity" class="headerlink" title="Regularization on Model Complexity"></a>Regularization on Model Complexity</h3><p>Recall that CART <script type="math/tex">g_t</script> partitions the input space into <script type="math/tex">J_{t}</script> disjoint regions <script type="math/tex">R_{1}^t,\ldots ,R_{J_{t}}^t</script> and predicts a constant value in each region</p>
<script type="math/tex; mode=display">g(\mathbf{x})=\sum_{j=1}^{J}b_j\mathbf {1} _{R_j}(\mathbf{x})</script><p>In XGBoost, we define the complexity as</p>
<script type="math/tex; mode=display">\Omega(g)=\eta J+\frac{1}{2}\lambda \sum_{j=1}^{J}b_j^2</script><p>First term $\eta J$ is the penalty on the number of leaves $J$, which is one source of complexity, and $\eta$ is a hyper-parameter to control the strength of this penalty. Complexity also comes from $b_j$ and the second term is the $L_2$ regularization on $b_j$, which is quite similar to Ridge regression. </p>
<p>Of course, there is more than one way to define the complexity, but this one works well in practice. The regularization is one part most tree packages treat less carefully, or simply ignore. This was because the traditional treatment of tree learning only emphasized improving impurity, while the complexity control was left to heuristics. </p>
<p>The objective function for boosting at each iteration can be modified as below</p>
<script type="math/tex; mode=display">\min_{\alpha_t, g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_tg_t(\mathbf{x}_i)) + \Omega(\alpha_tg_t)</script><p>A CART $g_t$ weighted by $\alpha_t$ is still a CART,</p>
<script type="math/tex; mode=display">\alpha_tg_t(\mathbf{x})=\sum_{j=1}^{J}\alpha_tb_j^t\mathbf {1} _{R_j^t}(\mathbf{x})</script><p><strong>Since in following Steps, we do not train a CART by information gain rule or any principle independent from loss function, we can cancel the weight $\alpha_t$</strong></p>
<script type="math/tex; mode=display">\min_{g_t}\sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)+g_t(\mathbf{x}_i)) + \Omega(g_t)</script><h3 id="Tayler-expansion-approximation"><a href="#Tayler-expansion-approximation" class="headerlink" title="Tayler expansion approximation"></a>Tayler expansion approximation</h3><p>One key specialty of XGboost is that it approximates this loss function by taking Taylor expansion of the loss function up to the second order to make <script type="math/tex">\alpha_t,g_t</script> independent from <script type="math/tex">f_{boost}^{t-1}</script></p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) \approx \sum_i^{n}L(y_i,f_{boost}^{t-1}(\mathbf{x}_i)) + L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>where <script type="math/tex">L^1=\frac{\partial L(y,\hat{y})}{\partial \hat{y}}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script> and <script type="math/tex">L^2=\frac{\partial L(y,\hat{y})}{\partial \hat{y}^2}|_{\hat{y}=f_{boost}^{t-1}(\mathbf{x}_i)}</script>. </p>
<p>As observed the first term is a constant, we can remove it.</p>
<script type="math/tex; mode=display">
\mathbf{L}(\mathbf{g_t|\mathbf{D}}) =\sum_i^{n} L^1g_t(\mathbf{x}_i) +\frac{1}{2}L^2(g_t(\mathbf{x}_i))^2 + \Omega(g_t)</script><p>One important advantage of this approximation is that the value of the objective function only depends on $L^1$ and $L^2$. This is how XGBoost <strong>supports custom loss functions</strong>. We can optimize every loss function, including logistic regression and pairwise ranking, using exactly the same solver that takes $L^1$ and $L^2$ as input!</p>
<h3 id="Gradient-Tree-Boosting"><a href="#Gradient-Tree-Boosting" class="headerlink" title="Gradient Tree Boosting"></a>Gradient Tree Boosting</h3><p>Given a known partitions ${R_j^t,\ j=1,\dots,J_t}$ of a CART $g_t$, take $g_t$ and $\Omega(g_t)$ in loss function above, we have</p>
<script type="math/tex; mode=display">\mathbf{L}(b_j^t|\mathbf{D},R_j^t) =\sum_{j=1}^{J_t}( b_j^t \sum_{\mathbf{x}_i \in R_j^t} L^1 +  (b_j^t)^2 \sum_{\mathbf{x}_i \in R_j^t}\frac{1}{2}L^2 + (b_j^t)^2\frac{1}{2}\lambda) + \eta J</script><p>This is a quadratic form with respect to $b_j^t$, and we can compute the optimal<br>weight $b_j^t$ of region $j$ in $g_t$</p>
<script type="math/tex; mode=display">b_j^t = -\frac{\sum_{\mathbf{x}_i \in R_j^t} L^1}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2}</script><p>Once we know the partition, we can always find the best weights by the formula above. If we take it back into the loss function, we can have</p>
<script type="math/tex; mode=display">\mathbf{L}(R_j^t|\mathbf{D}) = -\frac{1}{2} \frac{(\sum_{\mathbf{x}_i \in R_j^t} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_j^t}L^2} +\eta J_t</script><p>This is a function only replies on partition. This function(score) is like the impurity score for evaluating decision trees, except that <strong>it is derived for a wider range of objective functions (as long as we can have first and second order derivative)</strong>. </p>
<p>Now let`s go talk about the partition <script type="math/tex">\{R_j^t,\ j=1,\dots,J_t\}</script>. Similar to information gain rule in CART, <strong>we can use this score as impurity i.e. indicator for greedy approach to determine a split on region</strong> (leaf split). </p>
<p>For binary tree, assume that <script type="math/tex">R_{left}</script> and <script type="math/tex">R_{right}</script> are the instance sets of left and right nodes after the split <script type="math/tex">R_{parent}= R_{left}\cup R_{right}</script>. Then the loss reduction after the split is given by</p>
<script type="math/tex; mode=display">\mathbf{L}_{split}=  \frac{1}{2} [ \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} + \frac{(\sum_{\mathbf{x}_i \in R_{left}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{left}}L^2} -  \frac{(\sum_{\mathbf{x}_i \in R_{parent}} L^1)^2}{\lambda+ \sum_{\mathbf{x}_i \in R_{parent}}L^2}] - \eta</script><p>This also makes XGboost different from GBDT, <strong>not only the leaf of weight but also the partition of regions (leaves) are relevant to loss function</strong>.</p>
<img src="/2019/12/23/Ensemble/xgb.jpg" class="" width="500">
<h2 id="Adaptive-Boosting-Adaboost"><a href="#Adaptive-Boosting-Adaboost" class="headerlink" title="Adaptive Boosting (Adaboost)"></a>Adaptive Boosting (Adaboost)</h2><p>Adaboost was initially proposed for classification. In binary classification task where $y_i \in \lbrace -1,1 \rbrace$ and $\hat{y}=sign(g(\mathbf{x}_i))$, $g:\mathcal{X} \to [-1,1] $ . It sets loss function as</p>
<script type="math/tex; mode=display">L(y,\hat{y})=\exp(-y\hat{y})</script><p>which means higher loss for mis-classification and versus (only valid for binary case). For each iteration training k-th learner, the empirical loss function is</p>
<script type="math/tex; mode=display">\begin{aligned}
\mathbf{L}(\alpha_t,g_t|\mathbf{D})=&\sum_{i=1}^{n}\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i)+\alpha_t g_t(\mathbf{x}_i)))\\
=&\sum_{i=1}^{n}w_i^t\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\\
=&\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t +\exp(\alpha_t)\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>where <script type="math/tex">w_i^t=\exp(-y_i(f_{boost}^{t-1}(\mathbf{x}_i))</script>, which measures the performance of first $t-1$ ensemble on instance $i$. </p>
<h3 id="Calculate-alpha-t"><a href="#Calculate-alpha-t" class="headerlink" title="Calculate $\alpha_t$"></a>Calculate $\alpha_t$</h3><p>As observed, the effect of <script type="math/tex">\alpha_t</script> and <script type="math/tex">f_{boost}^{t-1}</script> to loss function is independent. Take the first order derivative we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial \mathbf{L}(\alpha_t,g_t|\mathbf{D})}{\partial \alpha_t}
=&-\exp(-\alpha_t)\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t + \exp(\alpha_t) \sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t
\end{aligned}</script><p>Set <script type="math/tex">\frac{\partial \mathbf{L}(y,g_t)}{\partial \alpha_t}=0</script> we have</p>
<script type="math/tex; mode=display">\begin{aligned}
\alpha_t
&=\frac{1}{2}\log(\frac{\sum_{y_i=g_t(\mathbf{x}_i)}w_i^t}{\sum_{y_i\ne g_t(\mathbf{x}_i)}w_i^t})\\
&=\frac{1}{2}\log(\frac{1-e_t}{e_t})
\end{aligned}</script><p>where $e_t$ is the <strong>weighted training error rate</strong> for $g_t$.</p>
<script type="math/tex; mode=display">e_t=\frac{\sum_{i=1}^{n}w_i^t\mathbf{1}(y_i=g_t(\mathbf{x}_i))}{\sum_{i=1}^{n}w_i^t}</script><p>Hense, if we know <script type="math/tex">g_t</script> we can get the <strong>closed form</strong> of optimal <script type="math/tex">\alpha_t</script>  directly and discard line searching. </p>
<h3 id="Weight-Update"><a href="#Weight-Update" class="headerlink" title="Weight Update"></a>Weight Update</h3><p>Once we add update the ensemble, we should update the weight for the next learner <script type="math/tex">g_{t+1}</script> as well. And as the formula shows, it only matters with the last update and the newly added learner performance which can save computation in practice.</p>
<script type="math/tex; mode=display">\begin{aligned}
f_{Adaboost}^{t}&=f_{Adaboost}^{t-1} + \alpha_tg_t(\mathbf{x})\\
w_i^{t+1}&=\exp(-y_if_{Adaboost}^{t})\\
&=\exp(-y_i(f_{Adaboost}^{t-1} + \alpha_tg_t))\\
&=w_i^{t}\times\exp(-y_i\alpha_tg_t(\mathbf{x}_i))\end{aligned}</script><h3 id="Train-g-k"><a href="#Train-g-k" class="headerlink" title="Train $g_k$"></a>Train $g_k$</h3><p>In practice, the weak learner may be an algorithm that can use the weights $w_i^t$ on the training examples. Alternatively, when this is not possible, a subset of the training examples can be sampled according to $w_i^t$, and these (unweighted) resampled examples can be used to train the weak learner. [<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwi0nvyBxd7mAhWNIjQIHeGdD9AQFjAAegQIBhAI&amp;url=https%3A%2F%2Fcseweb.ucsd.edu%2F~yfreund%2Fpapers%2FIntroToBoosting.pdf&amp;usg=AOvVaw21X5ZIA1LN2tzNBYDnISDR" target="_blank" rel="noopener">reference</a>]</p>
<p>Algorithms that can use weights:???</p>
<p>Compared with all previous algorithms, one special thing is we <strong>change the weight of training sample</strong> at each iteration of training weak leaner. In general, we assign more wight to samples mis-classified by last weak learner and versus. (<del>Q: Why weighting samples works?</del>A: cuz it was derived and reflected in loss function.)</p>
<img src="/2019/12/23/Ensemble/ada.jpg" class="" width="500">
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li><p>for t in 1 to K:<br> 2.1 train the learner $g_t$ with sample weighted by <script type="math/tex">w_i^t</script><br> 2.2 </p>
<ul>
<li>get the weighted training error rate $e_t$ (<del>mis-classification rate</del>) </li>
<li>calculate the weight for this learner <script type="math/tex; mode=display">\alpha_t=\frac{1}{2}\log(\frac{1-e_t}{e_t})</script></li>
</ul>
<p>2.3 update sample weight</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\left\{
\begin{aligned}
w_i^t e^{\alpha_i \times -1}, &\ if\ g_t(\mathbf{x}_i)=y_i\\
w_i^t e^{\alpha_i \times 1}, &\ else\\
\end{aligned}
\right.</script><p> if $\mathbf{x}_i$ is correctly classified by $g_t$ the weight should be shrunk by $e^{\alpha_i \times -1}$ otherwise we should emphasize this sample by multiplying $e^{\alpha_i \times 1}$ , else $w_i^t=w_i^{t-1}e^{\alpha_i \times 1}$<br> and then normalize the weight <script type="math/tex">w_i^{t+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script> </p>
</li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(X)</script></li>
</ol>
<h3 id="Regression-R2"><a href="#Regression-R2" class="headerlink" title="Regression (R2)"></a>Regression (R2)</h3><ol>
<li>Initialize: all training instance have equal sample weight <script type="math/tex">w_i^1=1/N_{train}</script></li>
<li>for t in 1 to K:<br> 2.1 train the learner <script type="math/tex">g_t</script> sample weighted by <script type="math/tex">w_i^t</script> and<br> 2.2 <ul>
<li>get the residuals <script type="math/tex">r_i^t = g_t((\mathbf{x}_i))-y_i</script> and the maximum abs value <script type="math/tex">r_{max}</script> </li>
<li>calculate the relative error <script type="math/tex">R_i^k = r_i^t/r_{max}</script> or <script type="math/tex">(r_i^t/r_{max})^2</script> or <script type="math/tex">1- \exp(- r_i^t/r_{max})</script> and take the weighted average as error rate <script type="math/tex; mode=display">e_t=\sum_i w_i^tR_i^t</script></li>
<li>Then calculate the weight<script type="math/tex; mode=display">\alpha_t=e_t/(1-e_t)</script>2.3 Update weights $w_i^{t+1}=w_i^k(\alpha_t)^{1-e_i^t}$ and normalize the weight  <script type="math/tex; mode=display">w_i^{k+1}=w_i^{t+1}/ \sum_i w_i^{t+1}</script></li>
</ul>
</li>
<li>get weighted average of all learners<script type="math/tex; mode=display">g_{Adaboost}=\sum_{t=1}^{K}\alpha_tg_t(\mathbf{x})</script></li>
</ol>
<p>Basically the process of training classification or regression model is same (at each iteration $t$, train $g_t$, calculate error rate $e_t$, calculate weight $\alpha_t$ for $g_t$ and update sample weights according to $g_t$). The only difference lies in how to calculate the error rate and weights. </p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>In order to avoid over-fitting, Adaboost also introduced regularization term, which is a shrinkage multiplier on base leaner weight</p>
<script type="math/tex; mode=display">\alpha_t=v\alpha_t</script><p>where $v\in (0,1)$. This shrinkage multiplier $v$ is also called learning rate.</p>
<h2 id="Boosting-Tree-Summary"><a href="#Boosting-Tree-Summary" class="headerlink" title="Boosting Tree Summary"></a>Boosting Tree Summary</h2><p>Briefly speaking, there are three types of boosting tree introduced above. All three use greedy approach to train a sequence of decision trees.</p>
<ul>
<li>GBDT(SGBDT)<ul>
<li>first order Taylor approximation on loss function</li>
<li>training with <strong>pseudo-residuals from predecessor</strong></li>
<li>training leaf weight according to loss function but requires <strong>line search</strong></li>
<li>training partition independently</li>
<li>introduced stochastic sampling as regularization</li>
</ul>
</li>
<li>XGboost<ul>
<li>A second order Taylor approximation is imposed on loss function</li>
<li>training with original data</li>
<li><strong>calculating </strong>leaf weight with a closed form (relevant to loss function)</li>
<li>training partition is dependent on loss function (first and second order derivative)</li>
<li>flexibility on regularization</li>
</ul>
</li>
<li>Adaboost<ul>
<li>fixed loss function that only makes sense in binary classification</li>
<li>training with <strong>weighted</strong> data</li>
<li>training leaf weight <strong>semi</strong> according to loss function ($\alpha_k^t$ is relevant to loss function)</li>
<li>training partition independently</li>
<li>introduced shrinkage parameter (learning rate) as regularization</li>
</ul>
</li>
</ul>
<p>In summary, only XGboost contains no independent training step. Independence of GBDT lies in partition while Adaboost also has independent leaves weight training. More flexibility on regularization can be achieved in XGboost as they emphasized in their paper (most boosting methods did not pay enough attention on regularization)</p>
<h2 id="LightBGM"><a href="#LightBGM" class="headerlink" title="LightBGM"></a>LightBGM</h2><h2 id="Catboost"><a href="#Catboost" class="headerlink" title="Catboost"></a>Catboost</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/28/Entropy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/28/Entropy/" class="post-title-link" itemprop="url">Gini Index vs Entropy</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-28 19:34:18" itemprop="dateCreated datePublished" datetime="2019-11-28T19:34:18-07:00">2019-11-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-07 22:14:43" itemprop="dateModified" datetime="2020-01-07T22:14:43-07:00">2020-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Gini Index</strong> and <strong>Entropy</strong> are two types of measure of <strong>impurity $I$</strong> (dispersion, inequality or variance) of <strong>categorical</strong> distribution which is used in information-gain rules in decision tree. Given $k$ categories and corresponding probability $p_k$, <strong>Gini Index</strong> (also called <a href="https://en.wikipedia.org/wiki/Gini_coefficient" target="_blank" rel="noopener">Gini coefficient</a>) is defined as below :</p>
<script type="math/tex; mode=display">I_{Gini}(p) = \sum_{i=1}^{k}p_i(1-p_i) = 1-\sum_{i=1}^{k}p_i^2 \in (0,1-\frac{1}{k})</script><p>while <strong>Entropy</strong> is defined as below:</p>
<script type="math/tex; mode=display">I_{Entropy}(p) = -\sum_{i=1}^{k}p_i\log(p_i) \in (0,1)</script><h1 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h1><ul>
<li>formula</li>
<li>relationship with impurity: The more impurity the distribution is, the larger either measure is.</li>
</ul>
<h1 id="Difference"><a href="#Difference" class="headerlink" title="Difference"></a>Difference</h1><ul>
<li>computing efficiency: Entropy is more computationally heavy due to the log in the equation. </li>
<li>sensitivity to tiny prob: Entropy is more sensitive to small prob (<0.2) and less sensitive to large prob (>0.2), which means <strong>sensitivity to noise</strong> at the same time.</li>
</ul>
<h1 id="Information-gain-rule"><a href="#Information-gain-rule" class="headerlink" title="Information-gain rule"></a>Information-gain rule</h1><p>Decision tree consists of sequence of decision nodes. In decision tree training algorithm, we recursively determine the decision nodes with respect to </p>
<ul>
<li>which feature or variable we use</li>
<li>where the decision boundary(s) for children nodes, i.e. where to split</li>
</ul>
<p>Information-gain rule is the guide for determining the decision nodes. <strong>Information-gain</strong> is defined as the difference of information (impurity) between all children nodes and their parent node. In other words, <strong>it measures how much information the splitting carries</strong>. Given the parent node $a$ is split into several children nodes $c_i$ by a splitting $T$, the Information-gain is defined as</p>
<script type="math/tex; mode=display">IG(T,a) = I(p(a)) - \sum_{i}\frac{|c_i|}{|p|}I(p(c_i))</script><p>where $|a|$ means the number of samples in node $a$ and $p(a)$ means the distribution of node $a$.</p>
<p>The idea of <strong>Information-gain rule</strong> is greedy algorithm. For each node, without other constrains(max depth, nothing to split), we use the splitting that results in <strong>maximum information gain</strong> and we recursively repeat layer by layer, node by node until all nodes contains no information (impurity is 0). It might stop early if there is constrain on the number of layers (max depth). For regression tree where the target variable is continuous, the impurity is measured by <strong>MSE</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/03/SSC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/SSC/" class="post-title-link" itemprop="url">SSC</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2019-11-03 17:25:03 / Modified: 19:34:03" itemprop="dateCreated datePublished" datetime="2019-11-03T17:25:03-07:00">2019-11-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">"C:\\Users\\jason\\Desktop\\SSC2019CaseStudy"</span>)</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Explore-the-cell-images"><a href="#Explore-the-cell-images" class="headerlink" title="Explore the cell images"></a>Explore the cell images</h1><h2 id="load-train-dataset"><a href="#load-train-dataset" class="headerlink" title="load train dataset"></a>load train dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df=pd.read_csv(<span class="string">'train_label.csv'</span>,index_col=<span class="string">'image_name'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>image_name</th>
      <th>count</th>
      <th>blur</th>
      <th>stain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>A01_C1_F1_s01_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s02_w1.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s02_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s03_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>A01_C1_F1_s04_w2.TIF</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,n,i+<span class="number">1</span>)</span><br><span class="line">    img_name=rd.choice(df.index)</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name)</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(img_name)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_3_0.png" class="" title="This is an image">
<h2 id="compare-img-with-different-cell-count"><a href="#compare-img-with-different-cell-count" class="headerlink" title="compare img with different cell count"></a>compare img with different cell count</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">img_name=[<span class="string">'A01_C1_F1_s13_w1.TIF'</span>,</span><br><span class="line">   <span class="string">'A02_C5_F1_s09_w1.TIF'</span>,</span><br><span class="line">    <span class="string">'A16_C66_F1_s04_w1.TIF'</span>]</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_5_0.png" class="" title="This is an image">
<h2 id="compare-img-with-different-level-of-F-and-w"><a href="#compare-img-with-different-level-of-F-and-w" class="headerlink" title="compare img with different level of F and w"></a>compare img with different level of F and w</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img_name = df.filter(like=<span class="string">'C14'</span>,axis=<span class="number">0</span>).filter(like=<span class="string">'s11'</span>,axis=<span class="number">0</span>).index</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">2</span>,<span class="number">2</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_7_0.png" class="" title="This is an image">
<h1 id="Linear-regression-with-grey-scale"><a href="#Linear-regression-with-grey-scale" class="headerlink" title="Linear regression with grey scale"></a>Linear regression with grey scale</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Regression <span class="keyword">import</span> PoolRegressor</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br></pre></td></tr></table></figure>
<h2 id="For-F1-w1-images"><a href="#For-F1-w1-images" class="headerlink" title="For F1 w1 images"></a>For F1 w1 images</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">F = <span class="string">'F1'</span></span><br><span class="line">w = <span class="string">'w1'</span></span><br><span class="line">X, df = utils.read_imgset(csv_path=<span class="string">'train_label.csv'</span>,train=<span class="literal">True</span>, F=F, w=w, hist = <span class="literal">True</span>)</span><br><span class="line">X, df = shuffle(X, df, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="cross-validation"><a href="#cross-validation" class="headerlink" title="cross validation"></a>cross validation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">kf = KFold(n_splits=<span class="number">10</span>)</span><br><span class="line">fit=PoolRegressor(pool= <span class="literal">False</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X[test,])</span><br><span class="line">    df[<span class="string">'pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  0.09604340485504105 +/- 0.006622262312150045
test mse =  2.389303273614496 +/- 0.33634267102092574
</code></pre><p>train mse is way smaller than test mse, <strong>overfitting alert</strong>!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_15_1.png" class="" title="This is an image">
<p>The error increase when more cells in the image</p>
<h2 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">img_name=[<span class="string">'A01_C1_F1_s13_w1.TIF'</span>,</span><br><span class="line">   <span class="string">'A02_C5_F1_s09_w1.TIF'</span>,</span><br><span class="line">    <span class="string">'A16_C66_F1_s04_w1.TIF'</span>]</span><br><span class="line">n=len(img_name)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    im = Image.open(<span class="string">'train/'</span>+img_name[i])</span><br><span class="line">    im=np.array(im)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(im,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.title(<span class="string">'count = '</span>+ str(df.loc[img_name[i],<span class="string">'count'</span>]))</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">2</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">1</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br><span class="line">    plt.subplot(n,<span class="number">3</span>,<span class="number">3</span>*i+<span class="number">3</span>)</span><br><span class="line">    plt.hist(im.reshape(<span class="number">-1</span>,<span class="number">1</span>),bins=range(<span class="number">0</span>,<span class="number">256</span>,<span class="number">15</span>))</span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'gray scale'</span>)</span><br></pre></td></tr></table></figure>
<img src="/2019/11/03/SSC/output_18_0.png" class="" title="This is an image">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fit=PoolRegressor(window=<span class="number">15</span>,step=<span class="number">15</span>,pool= <span class="literal">True</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'pooling_pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X[test,])</span><br><span class="line">    df[<span class="string">'pooling_pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  0.5545320752842191 +/- 0.01830920026615093
test mse =  0.6205896336762233 +/- 0.17798416968612663
</code></pre><p>Overfitting issue is solved by pooling (setting wider bins)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'pooling_pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_21_1.png" class="" title="This is an image">
<p><strong>heteroscedasticity</strong> the residuals still get larger as the prediction moves from small to large</p>
<p>How to Fix</p>
<ul>
<li>The most frequently successful solution is to <strong>transform</strong> a variable.</li>
<li>Often heteroscedasticity indicates that a <strong>variable is missing</strong>.</li>
</ul>
<h3 id="log-trasforamtion-failed"><a href="#log-trasforamtion-failed" class="headerlink" title="log trasforamtion (failed)"></a>log trasforamtion (failed)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X_log=np.log(X+<span class="number">1</span>)</span><br><span class="line">fit=PoolRegressor(window=<span class="number">15</span>,step=<span class="number">15</span>,pool= <span class="literal">True</span>)</span><br><span class="line">mse_train=[]</span><br><span class="line">mse_test=[]</span><br><span class="line">df[<span class="string">'log_pred'</span>]=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    ytrain= df[<span class="string">'count'</span>][train]</span><br><span class="line">    fit.train(X_log[train,], df[<span class="string">'count'</span>][train])</span><br><span class="line">    ypred=fit.predict(X_log[train,])</span><br><span class="line">    mse_train.append( mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][train]))</span><br><span class="line">    ypred=fit.predict(X_log[test,])</span><br><span class="line">    df[<span class="string">'log_pred'</span>][test]=ypred</span><br><span class="line">    mse_test.append(mean_squared_error(y_pred=ypred,y_true=df[<span class="string">'count'</span>][test]))</span><br><span class="line">print(<span class="string">'train mse = '</span>,np.mean(mse_train),<span class="string">'+/-'</span>, np.std(mse_train))</span><br><span class="line">print(<span class="string">'test mse = '</span>,np.mean(mse_test),<span class="string">'+/-'</span>, np.std(mse_test))</span><br></pre></td></tr></table></figure>
<pre><code>train mse =  137.2764500289679 +/- 6.5020079871442205
test mse =  196.17602353002354 +/- 132.57460287167424
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">residual = np.array(df[<span class="string">'count'</span>]) - np.array(df[<span class="string">'log_pred'</span>])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">2.5</span>))</span><br><span class="line">_ = ax.scatter(np.array(df[<span class="string">'count'</span>]),residual)</span><br><span class="line">plt.xlabel(<span class="string">'true count'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'residuals'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;residuals&#39;)
</code></pre><img src="/2019/11/03/SSC/output_25_1.png" class="" title="This is an image">
<p>Patterns like this indicate that a variable needs to be transformed you probably need to create a <strong>nonlinear</strong> model</p>
<h2 id="Topology-feature"><a href="#Topology-feature" class="headerlink" title="Topology feature"></a>Topology feature</h2><h2 id="overlapping-example"><a href="#overlapping-example" class="headerlink" title="overlapping example"></a>overlapping example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/03/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/hello-world/" class="post-title-link" itemprop="url">hello-world</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-03 16:49:19" itemprop="dateCreated datePublished" datetime="2019-11-03T16:49:19-07:00">2019-11-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Jiaxin Zhang"
    src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiaxin Zhang</p>
  <div class="site-description" itemprop="description">4+ years experience in programming. Familiar with python, SQL server, R, and Git. Highly motivated and humble to learn new things and learn fast with practice. Enjoy teamwork with strong sense of responsibility and communication skills, but also able to work individually with high efficiency, strong self-discipline and problem solving ability.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiaxin Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
