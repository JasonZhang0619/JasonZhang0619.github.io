<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Jiaxin Zhang" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Duality TheoryLagrangianGiven a standard form problem (not necessarily convex) \begin{aligned} {\rm min}_{x \in \mathcal{R}^p}\ f(x)&amp; \\ {\rm subject \ to\ } g_i(x)&amp;\le 0, i=1,\dots,n_g\\ h_i(x)&amp;=0, i">
<meta name="keywords" content="Machine learning, Optimization">
<meta property="og:type" content="article">
<meta property="og:title" content="Lagrange Duality and SVM">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;01&#x2F;13&#x2F;SVM&#x2F;index.html">
<meta property="og:site_name" content="Jiaxin Zhang">
<meta property="og:description" content="Duality TheoryLagrangianGiven a standard form problem (not necessarily convex) \begin{aligned} {\rm min}_{x \in \mathcal{R}^p}\ f(x)&amp; \\ {\rm subject \ to\ } g_i(x)&amp;\le 0, i=1,\dots,n_g\\ h_i(x)&amp;=0, i">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-01-16T03:38:07.434Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/01/13/SVM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Lagrange Duality and SVM | Jiaxin Zhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiaxin Zhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">MSc in Statistical Machine Learning @ UofA</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/JasonZhang0619" class="github-corner" title="GitHub" aria-label="GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/13/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiaxin Zhang">
      <meta itemprop="description" content="Curious, Open-eyed, Humble, Prepared, Sensitive.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiaxin Zhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Lagrange Duality and SVM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-13 16:15:17" itemprop="dateCreated datePublished" datetime="2020-01-13T16:15:17-07:00">2020-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-15 20:38:07" itemprop="dateModified" datetime="2020-01-15T20:38:07-07:00">2020-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Duality-Theory"><a href="#Duality-Theory" class="headerlink" title="Duality Theory"></a><a href="http://www.onmyphd.com/?p=duality.theory" target="_blank" rel="noopener">Duality Theory</a></h1><h2 id="Lagrangian"><a href="#Lagrangian" class="headerlink" title="Lagrangian"></a>Lagrangian</h2><p>Given a standard form problem (not necessarily convex)</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x \in \mathcal{R}^p}\ f(x)& \\
{\rm subject \ to\ } g_i(x)&\le 0, i=1,\dots,n_g\\
h_i(x)&=0, i=1,\dots,n_h
\end{aligned} \tag{1}</script><p>The problem, which is defined as the <strong>primal problem</strong>, has equality and inequality constraints. Denote the primal feasible area described by constraint functions <script type="math/tex">g_i</script> and <script type="math/tex">h_i</script> as </p>
<script type="math/tex; mode=display">\mathcal{D}=(\cap_{i=1}^{n_h}dom\ h_i)\cap(\cap_{i=1}^{n_g}dom\ g_i)</script><p>We define the Lagrangian <script type="math/tex">L:\mathcal{R}^p \times \mathcal{R}_+^{n_g} \times \mathcal{R}^{n_h} \to \mathcal{R}</script></p>
<script type="math/tex; mode=display">\mathbf{L}(x,\lambda,\nu)= f(x) + \sum_{i=1}^{n_h}\nu_ih_i(x)+  \sum_{i=1}^{n_g}\lambda_ig_i(x)</script><p>This is a weighted sum of objective $f$ and constraint functions $g_i$ and $h_i$, where $\lambda_i \ge 0$ is Lagrange multiplier associated with $g_i(x) \le 0$ and $\nu_i$ is Lagrange multiplier associated with $h_i(x)=0$. If we define the primal function $p(x)$ as follow</p>
<script type="math/tex; mode=display">
p(x) = sup_{\lambda\ge 0,\nu}\mathbf{L}(x,\lambda,\nu) = \left\{
\begin{aligned} f(x) \qquad & {\rm if} \ x \in \mathcal{D}\\
+\infty \qquad & {\rm else}
\end{aligned}
\right.
\tag{2}</script><blockquote>
<p>Proof: if $g_i(x)\le 0, h_i(x)=0$ any positive $\lambda_i$ makes Lagrangian less while $\nu_i$ does not effect the Lagrangian. Hence, the Sup of Lagrangian must be $f(x)$. On the other hand, if $g_i(x) \ge 0$ or $h_i(x)=0$ for any $i$, we can set corresponding weight as infinity then the Lagrangian can be infinity.</p>
</blockquote>
<p>Then we have </p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x}\ &f(x) \\
{\rm subject\ to\ }&x\in \mathcal{D}
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm min}_{x}\ &p(x) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned}</script><p>Hence, the optimization (1) is equivalent to minimizing (2), which in other words is:</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm min}_{x}\ &f(x) \\
{\rm subject\ to\ }&x\in \mathcal{D}
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm min}_{x}{\rm max}_{\lambda,\nu}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \tag{Primal problem}</script><p>The function $p(x)$ (or $f(x)$) is called <strong>primal function</strong> </p>
<h2 id="Duality"><a href="#Duality" class="headerlink" title="Duality"></a>Duality</h2><p>The <strong>primal problem</strong> maximize Lagrangian with respect to $\lambda,\nu$ first and then maximize with respect to $X$. We can define the <strong>dual problem</strong> by <strong>switching the order of minimize and maximize</strong></p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm max}_{\lambda,\nu}{\rm min}_{x}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned}</script><p>Now we define the <strong>dual function</strong>:</p>
<script type="math/tex; mode=display">q(\lambda,\nu)=\inf_x\mathbf{L}(x,\lambda,\nu)</script><p>The dual problem is equivalent to maximize dual function:</p>
<script type="math/tex; mode=display">\begin{aligned}
{\rm max}_{\lambda,\nu}\ &q(\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \leftrightarrows
\begin{aligned}
{\rm max}_{\lambda,\nu}{\rm min}_{x}\ &\mathbf{L}(x,\lambda,\nu) \\
{\rm subject\ to\ }&\lambda_i \ge 0
\end{aligned} \tag{Dual problem}</script><p>Then we have the the lower bound property:</p>
<script type="math/tex; mode=display">q(\lambda,\nu)=\inf_x\mathbf{L}(x,\lambda,\nu)\le \inf_{x\in \mathcal{D}}\mathbf{L}(x,\lambda,\nu) \le sup_{\lambda\ge 0,\nu}\mathbf{L}(x,\lambda,\nu) = f(x), x\in \mathcal{D}</script><p>It tells us that $q(λ,μ)$ is always smaller than $f(x)$ and, therefore, it serves as a lower bound for the <strong>primal function</strong>.</p>
<h2 id="Duality-Gap"><a href="#Duality-Gap" class="headerlink" title="Duality Gap"></a>Duality Gap</h2><p>Assume the optimal value of primal problem is $p^\star$. Given a dual feasible $\lambda,\nu$, we can establish a lower bound on the optimal value of primal problem: $p^\star \ge q(\lambda,\nu)$. Hence, we have</p>
<script type="math/tex; mode=display">0 \le f(x)-p^\star \le f(x) - q(\lambda,\nu)</script><p>Versus, assume optimal value of dual problem is $q^\star$, given a primal feasible $x$ we have</p>
<script type="math/tex; mode=display">0 \le q^\star - q(\lambda,\nu) \le f(x) - q(\lambda,\nu)</script><p>The difference $f(x) - q(\lambda,\nu)$ is called <strong>duality gap</strong> associated with the primal feasible point $x$ and dual feasible point $\lambda,\nu$ and is always non-negative. If we can find the primal dual feasible point $x^\star, \lambda^\star,\nu^\star$ such that the <strong>duality gap is zero</strong>, then $x^\star$ is primal optimal and $\lambda^\star,\nu^\star$ is dual optimal</p>
<blockquote>
<p>Brief proof. When duality gap is zero, $0 \le f(x^\star)-p^\star \le 0$. Hence $f(x^\star)=p^\star$</p>
</blockquote>
<p><strong>When the duality gap is zero, we say the problem has a strong duality, otherwise it has a weak duality. </strong> </p>
<blockquote>
<p>Strong duality does not hold in general but if the primal problem is convex i.e. of form with $f$ and $g_i$ convex, we usually (but not always) have strong duality. </p>
</blockquote>
<script type="math/tex; mode=display">\begin{aligned}
f(x^\star) &= q(\lambda^\star,\nu^\star) \\
&= {\rm inf}_{x}(f(x) + \sum_{i=1}^{n_h}\nu_ih_i(x)+  \sum_{i=1}^{n_g}\lambda_ig_i(x))\\
&\le f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star \underbrace{h_i(x^\star)}_{=0}+  \sum_{i=1}^{n_g}\underbrace{\lambda_i^\star}_{\ge 0} \underbrace{g_i(x^\star)}_{\le 0 }\\
&\le f(x^\star)
\end{aligned}</script><p>We can draw an important conclusion from above:</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n_g}\lambda_i^\star g_i(x^\star) =0</script><p>Since all terms are non-positive, we conclude that</p>
<script type="math/tex; mode=display">\lambda_i^\star g_i(x^\star) =0, \quad i=1,\dots,n_g \tag{2}</script><p>This term is known as complementary slackness; it holds any primal optimal $x^\star$ when strong duality holds. We can express this complementary slackness condition as </p>
<script type="math/tex; mode=display">\lambda_i^\star > 0 \to g_i(x^\star) =0</script><p>or equivalently,</p>
<script type="math/tex; mode=display">g_i(x^\star) < 0 \to \lambda_i^\star > 0</script><p>Roughly speaking, this means the $i$-th optimal Lagrange multiplier is zero unless the $i$-th constraint is active at the optimum.</p>
<p>Another conclusion we can draw is that since the third inequality is actually an equality, we conclude that the primal optimal $x^\star$ minimizes the Lagrange $L(x, \lambda^\star,\nu^\star)$ over $x$, which indicates that its gradient must vanish at $x^\star$</p>
<script type="math/tex; mode=display">\triangledown f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star\triangledown h_i(x^\star)+  \sum_{i=1}^{n_g}\lambda_i^\star\triangledown g_i(x^\star) =0 \tag{3}</script><h2 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h2><p>Summarize the conditions above we have: </p>
<script type="math/tex; mode=display">\begin{aligned}
g_i(x^\star) & \le 0, \quad i=1,\dots,n_g\\
h_i(x^\star) & = 0, \quad i=1,\dots,n_h\\
\lambda_i^\star &\ge 0, \quad i=1,\dots,n_g\\
\lambda_i^\star g_i(x^\star)& =0, \quad i=1,\dots,n_g\\
\triangledown f(x^\star) + \sum_{i=1}^{n_h}\nu_i^\star\triangledown h_i(x^\star)+  \sum_{i=1}^{n_g}\lambda_i^\star\triangledown g_i(x^\star) &=0
\end{aligned}</script><p>These are call <strong>Karush-Kuhn-Tucker (KKT) conditions</strong>. It consists of four parts:</p>
<ul>
<li>First two conditions comes from <strong>primal problem</strong></li>
<li>third one is <strong>non-negative constrain on Lagrange multiplier</strong>. </li>
<li>the 4-th condition follows equation (2) <strong>complementary slackness</strong>.</li>
<li>the last one follows equation (3) which is <strong>gradient</strong> vanishing constrain. </li>
</ul>
<p>The KKT conditions is necessary when the primal problem is non-convex and is also <strong>sufficient for points to be primal and dual optimal when when the primal problem is convex</strong>. </p>
<h1 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h1><p>Support Vector Machine (SVM) has achieved massive success and still keeps popular in many real-world situations especially in high-dimensional classification tasks. The main idea of SVM is to find the best hyperplane that separates two groups of data. So basically it is still a linear classifier. </p>
<p>Assuming $x_i\in \mathcal{R}^p$ and $y_i\in \lbrace -1,1 \rbrace$, a hyperplane was defined as </p>
<script type="math/tex; mode=display">\lbrace x:x^T\beta+\beta_0=0\rbrace</script><p>Then the classification function is </p>
<script type="math/tex; mode=display">g(x)=sign(x^T\beta+\beta_0)</script><p>The <strong>geometric margin</strong> from a point $(x_i,y_i)$ to the hyperplane is </p>
<script type="math/tex; mode=display">m_i = \frac{|x_i^T\beta+\beta_0|}{\|\beta\|}</script><p>$m_i$ measures the geometric distance from an observation to the hyperplane. </p>
<p>Hence, we aim to find the hyperplane maximizing the minimum geometric margin</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}m\\ \text{subject to } m_i\ge m,\ &i=1,...,n
\end{aligned}</script><p>where $m$ is the lower bound of geometric Margins. Alternatively, multiplied $||\beta||$  on both sides, the optimization problem is equivalent to the following form</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\begin{aligned}
&\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}m \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge m \|\beta\|
\end{aligned} 
\\
 m=\frac{M}{\|\beta\|} \downarrow &
\\
&\begin{aligned}
&\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{M}{\|\beta\|} \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge M,
\end{aligned}
\\
 M=1(i.e. \beta\leftarrow m \beta) \downarrow &
\\
&\begin{aligned}
&\min_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2 \\ 
&\text{subject to } |x_i^T\beta+\beta_0|\ge 1,
\end{aligned}
\end{aligned}</script><p>The definition of margin origins from the constrain $|x_i^T\beta+\beta_0|\ge 1$. The <strong>functional margin</strong> of an observation $(x_i,y_i)$ with respect to the hyperplane is defined as </p>
<script type="math/tex; mode=display">M_i=|x_i^T\beta+\beta_0|=y_i(x_i^T\beta+\beta_0)</script><p>and we have our optimization problem as follow:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2\\ \text{subject to } M_i\ge 1,\ &i=1,...,n
\end{aligned}\tag{Primal problem}</script><h2 id="Slackness"><a href="#Slackness" class="headerlink" title="Slackness"></a>Slackness</h2><p>When two clusters are not linearly separable or even overlap, a soft constrain is introduced to allow for some points on the wrong side of the hyperplane. As shown below, the optimization problem is modified by introducing the slack variables $\xi$.    </p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_{\beta \in \mathcal{R}^p,\beta_0 \in \mathcal{R}}\frac{1}{2}\|\beta\|^2\\ \text{subject to } M_i&\ge 1-\xi_i,\ i=1,...,n\\
\xi_i&\ge 0,\ i=1,...,n;\\
\sum_{i=1}^{n}\xi_i&\le F
\end{aligned}</script><p>where $F$ is a constant controlling the total amount of slackness we allow. By introducing slack variables $\xi$, we allow small margins close to the hyperplane with $\xi\in[0,1]$ and even negative margins i.e. $yf(x)&lt;0$ with $\xi\ge 1$.      </p>
<h2 id="Dual-problem"><a href="#Dual-problem" class="headerlink" title="Dual problem"></a>Dual problem</h2><p>Obviously, this is a convex problem with convex objective function and linear constrain over $\beta,\beta_0$. To find the solution, we firstly get the Lagrangian as follow</p>
<script type="math/tex; mode=display">
L((\beta_0,\beta,\xi),(C,\alpha,\mu))= \frac{1}{2}\|\beta\|^2-\sum_{i=1}^{n}\alpha_i(M_i-(1-\xi_i))-\sum_{i=1}^{n}\mu_i\xi_i+C\sum_{i=1}^{n}\xi_i</script><p>where $\alpha_i,\mu_i,C$ are Lagrange multiplier for the constrains and $\xi_i$ are feasible variables and all of them must be positive. Since the constant is independent of any variables, we usually drop the constant term.</p>
<p>Then the primal problem is </p>
<script type="math/tex; mode=display">\min_{\beta_0,\beta,\xi}\max_{C,\alpha,\mu}L((\beta_0,\beta,\xi),(C,\alpha,\mu))</script><p>The corresponding <strong>Dual problem</strong> is </p>
<script type="math/tex; mode=display">\max_{C,\alpha,\mu}\min_{\beta_0,\beta,\xi}L((\beta_0,\beta,\xi),(C,\alpha,\mu))</script><p>By setting the gradient of Lagrangian w.r.t $\beta_0,\beta,\xi$ as 0 we have:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\triangledown_{\beta_0} L =0 &\to  0=\sum_{i=1}^{n}\alpha_iy_i,\\
\triangledown_{\beta} L =0 &\to \beta=\sum_{i=1}^{n}\alpha_iy_ix_i,\\
\triangledown_{\xi_i} L =0 &\to \alpha_i=C-\mu_{i},
\end{aligned}</script><p>we can cancel $\beta,\beta_0, \xi_i$ and also $\mu_i, C$ obtain the Lagrangian as below</p>
<script type="math/tex; mode=display">
\begin{aligned}
L((\beta_0,\beta,\xi),(C,\alpha,\mu)) 
&= \frac{1}{2}\|\beta\|^2
-\sum_{i=1}^{n}\alpha_i(M_i-(1-\xi_i))
-\sum_{i=1}^{n}\mu_i\xi_i
+C\sum_{i=1}^{n}\xi_i\\
&=\frac{1}{2}\|\sum_{i=1}^{n}\alpha_iy_ix_i\|^2 
-\sum_{i=1}^{n}\alpha_i(y_i(x_i^T(\sum_{j=1}^{n}\alpha_jy_jx_j)+\beta_0)-(1-\xi_i))
+\sum_{i=1}^{n}(C-\mu_i)\xi_i\\
&=\frac{1}{2}\|\sum_{i=1}^{n}\alpha_iy_ix_i\|^2
 -\sum_{i=1}^{n}\alpha_i(y_ix_i^T(\sum_{j=1}^{n}\alpha_jy_jx_j)
 -\sum_{i=1}^{n}\alpha_iy_i\beta_0
 +\sum_{i=1}^{n}\alpha_i
 -\sum_{i=1}^{n}\alpha_i\xi_i
 +\sum_{i=1}^{n}\alpha_i\xi_i\\
&=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j +\sum_{i=1}^{n}\alpha_i\\
&=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j
\end{aligned}</script><p>then <strong>Dual problem</strong> is as below</p>
<script type="math/tex; mode=display">
\max_{\alpha,\mu,C} \sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
\text{subject to}\sum_{i=1}^{n}\alpha_iy_i=0</script><p>where the only variable is $\alpha$. Assume we find the unique solution $\alpha_i^*$ for the dual problem. The slope for hyperplane is </p>
<script type="math/tex; mode=display">\beta^\star=\sum_{i=1}^{n}\alpha_i^\star y_ix_i</script><p>Notice that the slope is only determined by a linear combinations of $y_ix_i$ and those observations with non-zero weights $\alpha_i\ne 0$ is called support vectors.</p>
<p>To determine a the hyperplane we also <a href="https://zhuanlan.zhihu.com/p/62420593" target="_blank" rel="noopener">require the intercept</a> $\beta_0^*$</p>
<h2 id="Sequential-minimal-optimization-SMO"><a href="#Sequential-minimal-optimization-SMO" class="headerlink" title="Sequential minimal optimization (SMO)"></a>Sequential minimal optimization (SMO)</h2><h2 id="KKT-conditions"><a href="#KKT-conditions" class="headerlink" title="KKT conditions:"></a>KKT conditions:</h2><p>Now we check the KKT conditions:</p>
<ul>
<li>constrains comes from <strong>primal problem</strong><script type="math/tex; mode=display">\begin{aligned}
M_i-(1-\xi_i)&\ge 0\\
\xi_i&\ge 0\\
\sum_{i=1}^{n}\xi_i-F&\le 0
\end{aligned}</script></li>
<li><strong>non-negative constrain on Lagrange multiplier</strong>.<script type="math/tex; mode=display">\begin{aligned}
\alpha_i&\ge 0\\
\mu_i&\ge 0\\
C &\ge 0
\end{aligned}</script></li>
<li><strong>complementary slackness</strong>.<script type="math/tex; mode=display">\begin{aligned}
\alpha_i(M_i-(1-\xi_i))&=0\\
\mu_i\xi_i&=0\\
C(\sum_{i=1}^{n}\xi_i-F)&=0\\
\end{aligned}</script></li>
<li><strong>gradient</strong> vanishing constrain. (must be true by setting gradient as zero)<script type="math/tex; mode=display">
\begin{aligned}
\beta_0: 0&=\sum_{i=1}^{n}\alpha_iy_i,\\
\beta_i: \beta&=\sum_{i=1}^{n}\alpha_iy_ix_i,\\
\xi_i: \alpha_i&=C-\mu_{i},
\end{aligned}</script></li>
</ul>
<p>Ideally we can use the 6 equations from <strong>complementary slackness</strong> and <strong>gradient</strong> vanishing constrain to <strong>find the one and only optimal solution and verify the other inequalities</strong>, however, it is hard to solve these equations. Although it seems <strong>impossible to get a closed form of optimal solution</strong>, which is why we search it by SMO with iterations, (so far I don’t know how to solve these equations), we can tell that with <strong>six equations and six variables, there must be a solution</strong>. Assuming the solution also follows the inequalities, we can draw the conclusion that the optimal solution we get for dual problem is also for primal problem.</p>
<p>Actually, it is <strong>not necessary to verify the KKT conditions</strong> since <strong>even if it is not satisfied, the optimal solution of dual problem is still a good estimation of primal problem.</strong></p>
<h1 id="Kernal-trick"><a href="#Kernal-trick" class="headerlink" title="Kernal trick"></a>Kernal trick</h1><p>So far the optimization is still searching for a linear classifier with the hyperplane defined as ${x:\ f(x)=x^T\beta+\beta_0=0}$ a linear from with respect to x. The “Kernel trick” makes it possible for linear boundaries to lie in a higher dimensional space and to be nonlinear projected on original space. </p>
<p>We define the basis functions as $h<em>m(x)=,m=1,…,M$ and represent all observations in the new space $h(x_i)=(h_1(x_i),h_2(x_i),…h_M(x_i))$. The hyperplane in this new space is ${x:\ f’(x)=f(h(x))=\sum</em>{i=1}^{n}\alpha_i^<em>y_ih(x_i)^Th(x) + \beta_0^</em>=0}$ and the classification function is still $g(x)=sign(f’(x))$. As observed from the form of hyperplane, the equation is a linear combination of the inner product of $h(x_i)$ and $h(x)$, which is called kernel function.</p>
<script type="math/tex; mode=display">
\mathcal{K}(x,y)=h(x)^Th(y)</script><p>There are many types of kernel functions such as polynomial, Gaussian, radial basis and splines. </p>
<h1 id="Why-Dual-Problem"><a href="#Why-Dual-Problem" class="headerlink" title="Why Dual Problem?"></a>Why Dual Problem?</h1><ul>
<li>Dual problem has no inequality constrain</li>
<li>The complexity of primal problem is proportional to dimension while that of dual problem only matters with sample size $n$</li>
<li>Kernel trick makes dimension even higher.</li>
</ul>
<h1 id="Multi-classification"><a href="#Multi-classification" class="headerlink" title="Multi-classification"></a>Multi-classification</h1><p>SVM is also capable of multi-classification and there are three ways based on binary classifiers: <code>one-against-all&#39;&#39;,</code>one-against-one’’, and  Directed Acyclic Graph Support Vector Machines (DAGSVM) . The first one builds $K$ SVM classifiers for $K$ classes respectively with data from $k$-th class as positive and from other classes as negative. Then we take the margins as score and we classify a new observation with the largest margin. The second method constructs $K(K - 1)/2$ classifiers where each one is trained on data from two classes pair-wisely. For a new observation, we use the voting strategy: with votes for all classes initialized as zero, we add one to $k$-th class’s vote if the result from one of these classifiers is $k$-th class, then the conclusion is the class with greatest votes. Though shares the same training steps as ``one-against-one’’ method with $K(K - 1)/2$ SVM classifiers, the third one DAGSVM is more complicated and delicate with a tree based decision function.</p>
<p>One thing we need to pay attention to is its sensitivity to prior. SVM is sensitive to both noise and prior. It tends to label the new observation into the class with more samples. This is somehow undesirable when we care more about the minority instead of majority. This property is illustrated in simulation where we checked the sensitivity of all methods we used.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-learning-Optimization/" rel="tag"># Machine learning, Optimization</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/01/07/square-root/" rel="next" title="Methods of computing square roots">
                  <i class="fa fa-chevron-left"></i> Methods of computing square roots
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/02/02/changba/" rel="prev" title="Scraping Changba data with python + selenium">
                  Scraping Changba data with python + selenium <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Duality-Theory"><span class="nav-number">1.</span> <span class="nav-text">Duality Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lagrangian"><span class="nav-number">1.1.</span> <span class="nav-text">Lagrangian</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Duality"><span class="nav-number">1.2.</span> <span class="nav-text">Duality</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Duality-Gap"><span class="nav-number">1.3.</span> <span class="nav-text">Duality Gap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KKT-Conditions"><span class="nav-number">1.4.</span> <span class="nav-text">KKT Conditions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Support-Vector-Machine"><span class="nav-number">2.</span> <span class="nav-text">Support Vector Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Slackness"><span class="nav-number">2.1.</span> <span class="nav-text">Slackness</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dual-problem"><span class="nav-number">2.2.</span> <span class="nav-text">Dual problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequential-minimal-optimization-SMO"><span class="nav-number">2.3.</span> <span class="nav-text">Sequential minimal optimization (SMO)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KKT-conditions"><span class="nav-number">2.4.</span> <span class="nav-text">KKT conditions:</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kernal-trick"><span class="nav-number">3.</span> <span class="nav-text">Kernal trick</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-Dual-Problem"><span class="nav-number">4.</span> <span class="nav-text">Why Dual Problem?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-classification"><span class="nav-number">5.</span> <span class="nav-text">Multi-classification</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Jiaxin Zhang"
    src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiaxin Zhang</p>
  <div class="site-description" itemprop="description">Curious, Open-eyed, Humble, Prepared, Sensitive.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiaxin Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
